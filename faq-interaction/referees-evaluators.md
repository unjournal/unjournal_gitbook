# Evaluation (refereeing)

## Evaluation guidelines and criteria

{% hint style="info" %}
[We refer to 'evaluation'](#user-content-fn-1)[^1] because The Unjournal does not _publish_ work; it only links, rates, and evaluates it.
{% endhint %}

_What we are asking evaluators/referees to do:_ [Guidelines for Evaluators](../policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/)

## Choosing and working with evaluators (referees)

### How do we choose evaluators?

* We follow standard procedures, considering complementary expertise, interest, cross-citations, and lack of conflict-of-interest. (See our internal guidelines for [choosing evaluators](../management-tech-details-discussion/management-process/choosing-evaluators/))
* We aim to consult those who have [opted-in to our referee pool](../benefits-and-features/) first
* We will favor evaluators with a track record of careful, in-depth, and insightful evaluation (while giving ECRs a chance to build such a record)

### Why do we pay evaluators?

* It is equitable, especially for those not getting 'service credit' for their refereeing work from their employer.
* While researchers currently write reports for prominent traditional journals for free, perhaps in exchange for goodwill when they submit their own work, or a desire to impress prominent editors...
  1. We need to use explicit incentives as the Unjournal grows.
  2. Paying evaluators can reduce adverse selection and conflicts of interest (potentially inherent to the traditional process).
* We can use these as incentives for high-quality work.
* We can use payments to access a wider range of expertise, including people not interested in submitting their own work to The Unjournal.

<details>

<summary>To claim your evaluator payment...</summary>

[#submitting-and-paying-expenses-claims](../management-tech-details-discussion/fiscal-hosting-and-expenses.md#submitting-and-paying-expenses-claims "mention")

</details>

##

## Evaluator concerns (anonymity, reputation, etc.)

### Anonymity/blinding vs. signed reports

_**Can I submit an evaluation anonymously?**_

Yes we allow evaluators to choose whether they wish to remain anonymous or 'sign their evaluations'.

### _**How do we protect the anonymity of evaluators who request it?**_

See [protecting-anonymity.md](../policies-projects-evaluation-workflow/evaluation/protecting-anonymity.md "mention")

### I'm concerned about making my evaluation public; what if I make an error or write something in a way that I later regret?

In the typical journal process, reviewers may make mistakes in reports, and these may typically be mitigated by multiple reviewers and editor mediation. Furthermore, in standard journals, reviews are typically not made public. As an Unjournal evaluator, you might be concerned about having an error or poor judgment enter the public record. _How worried should you be, and what can be done to limit this concern?_

1. You can choose to make your evaluation anonymous. You can make this decision from the outset (this is preferable), or later after you've completed your review. This is your choice.
2. Your evaluation will be shared with the authors before it is posted, and they will be given two weeks to respond before we post this. If they cite what they believe are any major misstatements in your evaluation, we will give you the chance to correct these.
3. It is well-known that referee reports/evaluations are subject to mistakes. We expect most people who read your evaluation will take this into account.
4. You can add an addendum/revision to your evaluation later on (see below).

### Can I redact my evaluation/review after it's published through the Unjournal?

We will put your review/evaluation on [PubPub](https://unjournal.pubpub.org) and give it a DOI. It cannot be redacted in the sense that this initial version will remain on the internet in some format. But you can certainly add an addendum to the review later, which we will post and link, and the DOI can be adjusted to point to the revised version.

### (How) are the research authors involved in Unjournal's review process and do they give consent?

See [for-researchers-authors](for-researchers-authors/ "mention") FAQ as well as the [direct-evaluation-track.md](../policies-projects-evaluation-workflow/considering-projects/direct-evaluation-track.md "mention").

We currently (May 2023) have two ways that papers/research projects enter the Unjournal process:

1. Authors submit their work (perhaps after our reaching out to them); if we believe it is relevant, we assign evaluators, etc. We can also agree with authors to 'embargo' evaluations until a later date, under certain conditions (and evaluators will be informed of this.)
2. Alternatively, we are selecting a set of working papers released in the [prominent NBER series](https://www.nber.org/papers?page=1\&perPage=50\&sortBy=public\_date) for evaluation [(see note)](#user-content-fn-2)[^2]; where these papers seem particularly influential, potentially impactful, and relevant for evaluation. For these, we contact the authors before sending out the papers for evaluation, and request the authors' engagement, but we don't ask for _permission_

For either track, authors are invited to be involved in several ways:

* They are informed of the process, and given an opportunity to identify particular concerns, request an embargo, etc.
* Evaluators can be put in touch with authors (anonymously) for clarification questions
* The authors are given a two-week window to respond to the evaluations (and the response gets published as well) before the evaluations are made public. They can also respond on our platform _after_ these are released.

### Can I share this evaluation? What else can I do with this?

If you are writing a signed (not anonymous) review, you can share it or link it on your own pages. Please wait to do this until _after_ we have given the author a chance to respond and posted the package (note we as of May 2023 put it up on our [PubPub](https://unjournal.pubpub.org) with a DOI and try to get it to scholarly search engines and bibliometric databases.)

Otherwise, if you are remaining anonymous. please do not disclose your connection to this report.

\
_Going forward:_

* We may later invite you to write and evaluate more about this piece of research
* ... and to help us judge prizes (e.g., the [impactful-research-prize](../readme/call-for-participants-research/impactful-research-prize/ "mention")).
* We may ask if you want to be involved in replication exercises (e.g., through the [Institute for Replication](https://i4replication.org/))
* We hope and intend to always see that you are fairly compensated for your time and effort, as a general principle

## The value of these evaluations; what to prioritize?

**What value do these evaluations provide (and how should evaluators think about this)? Who is the audience? How much is this as a 'service for authors'?**

The evaluations provide at least three types of value, helping advance several paths in our [theory of change (see flowchart)](../benefits-and-features/global-priorities-theory-of-change/)

1. **For readers and users**: Unjournal evaluations assess the reliability and usefulness of the paper along several dimensions, and to make this public, so other researchers and policymakers can [learn from this and this in their practice and decisionmaking.](#user-content-fn-3)[^3]
2. **For careers and improving research:** Evaluations provide metrics of quality. In the medium term, these should provide career value in a better way, improving the research process. We aim to build metrics that are credibly comparable to the current ‘what tier journal is a paper published in’. But we aim to do this better:
   * More quickly, more reliably, more transparently, and without the unproductive overhead of dealing with journals (see '[reshaping evaluation](../benefits-and-features/costs-of-playing-the-publication-game.md)')
   * Allowing flexible [transparent formats (such as dynamic documents)](../benefits-and-features/dynamic-documents-vs-living-projects.md)... improving the research process, research careers, and hopefully improving the research itself in impactful areas.
3. **Feedback and suggestions for authors:** We also hope that evaluators provide feedback that is relevant to the authors, to help them make the paper better.

<details>

<summary>Is 'feedback for authors' more important for Unjournal evaluations than for traditional journals?</summary>

In the near term, while an UJ evaluation may not yet seen to have substantial career value...

* Work UJ considers might tend be at an earlier stage relative to papers submitted to journals, as authors who submit work may see this as ‘pre-journal’,
* and the papers we select (from NBER atm) might be posted before authors are submitting them to journals
* On the other hand, we also tend to consider (as of July 2023) a lot of NBER papers that seem close to being submitted (or are currently in submission) at top traditionaljournals.

\
Medium-term, if a positive UJ evaluation ‘has value’, it may be equally likely that the UJ evaluation is seen as an ‘endpoint’

Also note that the modal outcome from submitting to a journal is rejection, so feedback is equally desirable from them.

</details>

<details>

<summary>Can The Unjournal 'do feedback to authors better' than traditional journals?</summary>

**Maybe we can?**

* We pay evaluators
* The evaluations are public, and some sign their evaluations
  * → evaluators may be more motivated to be careful and complete

**On the other hand**

* For public evaluations, people might defer to being overly careful
* At standard journals, referees do want to impress editors, and often (but not always) leave very detailed comments and suggestions (

</details>

Given the above considerations, and considering that we want to encourage authors to see value in engaging with The Unjournal, we encourage evaluators to prioritize the three components roughly equally:

* Making the evaluations and ratings useful for readers and users
* Making them meaningful for assessing academics
* Communicating useful feedback and suggestions to researchers\


## Public evaluations

### How are these hosted and shared?

(2023 update)

* On our [PubPub page](https://unjournal.pubpub.org/).
  * Each evaluations and response gets a DOI, linking these into all relevant systems, including Google Scholar
* Previously: [Kotahi](https://kotahi.community/), linked to a [Sciety](https://sciety.org/) group (we aim to mirror PubPub content to our Sciety group and vice-versa)
* Potentially: Hosted/mirrored on our own dedicated web page
* We will present the 'ratings data' in clear, comparable formats, as well as providing the raw data for meta-analysts and others (this is partially available in the PubPub content, and we are working on making this more systematic)

## General criteria/guidelines

* See our [guidelines for evaluators](../policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/) for our current (pilot-stage) approach and template for referees.
* See also: "[Considering Papers](../policies-projects-evaluation-workflow/considering-projects/)"

1. E.g., 'Selection': Traditionally, reviewers may be more likely to accept an assignment when they have a particular interest in the paper under consideration
2. E.g., for wider audiences, e.g., on Wikipedia, the EA Forum, or Asterisk magazine, with potential further compensation.
3. 22 Dec 2022: We are still developing and improving this system.

[^1]: Still, we sometimes call the evaluators 'referees' for clarity.

[^2]: Note that papers in the NBER series may enter through either of the above tracks; we will explain this to evaluators.

[^3]: They can better understand how to use it, what parts to use, and how much to believe it applies in different contexts, etc.

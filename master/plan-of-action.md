---
description: >-
  My (David Reinstein's) proposed 'best feasible plan' for what we (RP & fellow
  travelers) should do right now and going forward to make this happen. I'll
  continue to update this as things develop,
---

# Plan of action: 'unjournal'

{% hint style="warning" %}
https://app.gitbook.com/o/-MfFk4CTSGwVOPkwnRgx/s/-MkORcaM5xGxmrnczq25/readme/plan-of-actionWhat is this Unjournal?... See:\
[#in-a-nutshell](../#in-a-nutshell "mention")
{% endhint %}

<details>

<summary>What we need our pilot (~12 months) to demonstrate</summary>

1. We actually **'do something'**
2. We can provide **credible reviews and ratings** that have value as measures of research quality comparable to (or better than) traditional journal systems
3. We identify important work that **informs global priorities**
4. We boost work in innovative and transparent/replicable formats (especially **dynamic documents**)
5. **Authors engage** with our process and find it useful
6. (As a push) Universities, grantmakers, and other arbiters assign value to Unjournal ratings \\

</details>

## Building EA-aligned research 'unjournal'

See [acx-ltff-grant-proposal-as-submitted-successfull](../grants-and-proposals/acx-ltff-grant-proposal-as-submitted-successfull/ "mention") for proposed specifics

### Setup and team

[action-build-founding-committee.md](../action-and-progress/action-build-founding-committee.md "mention")

**Define the (broad) scope** of our research interest and **key overriding principles**. Light-touch, to also be attractive to aligned academics

Build **"editorial-board-like"** teams with subject/area expertise

Determine our relative needs, funds, and resources (especially to pay reviewers; see grants awarded and applications below)



### Create a set of rules for 'submission and management'

* which projects enter the review system (relevance, minimal quality, stakeholders, any red lines or 'musts')
* how projects are to be submitted (see above, but let's be flexible)
* how reviewers are to be assigned and compensated (or 'given equivalent credit' in system of accounts)

### Rules for reviews/assessments

* To be done on the chosen open platform (~~Prereview~~ probably Kotahi/Sciety) unless otherwise infeasible
* Share, advertise, promote this, have efficient meetings and presentations
  * Establish links to all open-access bibliometric initiatives (to the extent feasible)
* Harness and encourage additional tools for quality assessment, considering cross-links to prediction markets/Metaculus, to coin-based 'ResearchHub', etc.

### Further steps

See [#the-twelve-month-plan](../grants-and-proposals/acx-ltff-grant-proposal-as-submitted-successfull/#the-twelve-month-plan "mention")

<details>

<summary>Key next steps (pasted from FTX application)</summary>

The key elements of the plan:

Build a ‘founding committee’ of 5-8 experienced and enthusiastic EA-aligned/adjacent researchers at EA orgs, research academics, and practitioners (e.g., draw from speakers at recent EA Global meetings).

1. Host a meeting (and shared collaboration space/document), to come to a consensus/set of practical principles
2. Post and present our consensus (coming out of this meeting) on key fora. After a brief ‘followup period’ (\~1 week), consider adjusting the above consensus plan in light of the feedback, and repost (and move forward).
3. Set up the basic platforms for posting and administering reviews and evaluations and offering curated links and categorizations of papers and projects. ~~Note: I am strongly leaning towards https://prereview.org/ as the main platform, which has indicated willingness to give us a flexible ‘experimental space~~’ Update: Kotahi/Sciety seems a more flexible solution
4. Reach out to researchers in relevant areas and organizations and ask them to 'submit' their work for 'feedback and potential positive evaluations and recognition', and for a chance at a prize. The Unjournal will _not be an exclusive outlet._ Researchers are free to also submit the same work to 'traditional journals' at any point. Their work must be publicly hosted, with a DOI. Ideally the 'whole project' is maintained and updated, with all materials, in a single location.

</details>

<details>

<summary>Some specific next actions (pasted from FTX application}</summary>

1. Post and present our consensus (coming out of this meeting) on key fora. After a brief ‘followup period’ (\~1 week), consider adjusting the above consensus plan in light of the feedback, and repost (and move forward).
2. Set up the basic platforms for posting and administering reviews and evaluations and offering curated links and categorizations of papers and projects. Note: I am strongly leaning towards https://prereview.org/ as the main platform, which has indicated willingness to give us a flexible ‘experimental space’
3. Reach out to researchers in relevant areas and organizations and ask them to 'submit' their work for 'feedback and potential positive evaluations and recognition', and for a chance at a prize. The Unjournal will _not be an exclusive outlet._ Researchers are free to also submit the same work to 'traditional journals' at any point. Their work must be publicly hosted, with a DOI. Ideally the 'whole project' is maintained and updated, with all materials, in a single location.

</details>

### Aside: 'Academic-level' work for EA research orgs

(building on [post at "onscienceandacademia.org"](https://onscienceandacademia.org/t/moving-science-beyond-closed-binary-static-journals-a-proposed-alternative-how-the-effective-altruist-and-nontraditional-nonprofit-sector-can-help-make-this-happen/1490))

_The approach below is largely integrated into the Unjournal proposal, but this is a suggestion for how organizations like RP might consider 'how to get feedback and boost credibility_

<details>

<summary>Proposed approach</summary>

1. **Host article** (or dynamic research project or 'registered report') on OSF or other place allowing time stamping & DOIs (see [my resources list in Airtable](https://airtable.com/shraTY0WcwsjJSANs) for a start)
2. Link this to [PREreview](https://prereview.org/reviews) (or similar tool) tools/sites soliciting feedback and evaluation without requiring exclusive publication rights... (again, see [Airtable list](https://airtable.com/shraTY0WcwsjJSANs))
3. **Directly solicit feedback** from EA-adjacent partners in academia and other EA-research orgs

* We need to **build our own systems** (assign ‘editors') to do this without bias and with incentives
* building standard metrics for interpreting these reviews (possibly incorporating prediction markets,
* encouraging them to leave their feedback through the PREreview or another platform.

Also: Committing to publish academic reviews or ‘share in our internal group’ for further evaluation and reassessment/benchmarking of the ‘PREreview’ type reviews above. (Perhaps taking the [FreeOurKnowledge pledge relating to this](https://github.com/FreeOurKnowledge/website/issues/40))

</details>

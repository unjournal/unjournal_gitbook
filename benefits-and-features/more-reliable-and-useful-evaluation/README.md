# Open, reliable and useful evaluation

## _Open_ evaluation (and rating)

Traditional peer review is a closed process, with reviewers' and editors' comments and recommendations hidden from the public.

In contrast, a[ll Unjournal evaluations\*](#user-content-fn-1)[^1] (along with authors' responses and evaluation manager summaries) are made public and easily accessible. We give each of these a separate DOI and work to make sure these each enter the literature and bibliometric databases. We aim to further curate these, making it easy to see the evaluators' comments in the context of the research project (e.g., with sidebar/hover annotation).

Open evaluation is more useful:

* To other researchers and students (especially those early in their careers). Seeing the dialogue helps them digest the research itself and see its relationship to the wider field. It helps them understand the strengths and weaknesses of the methods and approaches used, and how much agreement there is over these. It gives an inside perspective on how evaluation works
* To people _using_ the research, providing further perspectives of its value, strengths and weaknesses, implications, and on how and where it might be used.

\
Publicly posting evaluations and responses may also make them of higher quality and more reliable. Evaluators can choose whether or not they wish to remain anonymous (and there are [pros and cons to each](broken-reference/)). In either case, the fact that all the content is _public_ may encourage evaluators to fully and transparently express their reasoning and justifications. (And where they fail to do so, people reading this can take this into account in considering the evaluation.)

The fact that we are asking for evaluations and ratings of all the projects in our system and not 'accept reject' should also drive more careful and comprehensive evaluation and feedback. At a traditional top-ranked journal, a reviewer may limit themselves to a few vague comments implying that the paper is 'not interesting or strong enough to merit publication'. This would not make sense within the context of The Unjournal's process.

## More reliable, precise, and useful _metrics_

We do not 'accept or reject' papers; we are _evaluating_ research, not 'publishing it'. But then, how do other researchers and students know whether it's worth reading? How can policymakers know whether to trust it? How can it help a researcher advance their career? How can grantmakers and organizations know whether to fund more of this research?

As an alternative to the traditional 'what tier did a paper get published in' measure, The Unjournal provides _metrics:_ We ask evaluators to provide a specific set of ratings and predictions about _aspects_ of the research, as well as aggregate measures. We make these public. We aim to synthesize and analyze these in useful ways, as well as make this quantitative data accessible to meta-science researchers, meta-analysts, and tool-builders.

See our [ratings metrics](../../policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/#metrics-overall-assessment-categories) and [prediction metrics](../../policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators/#journal-prediction-metrics) (these are our pilot metrics, we aim to refine these).

These are separated into different categories, designed to help researchers, readers, and users understand things like:

* How much to believe the results stated by the authors (and why/why not)?
* How relevant are these results for particular real-world choices and considerations?
* Is the paper written in a way that is clear and readable?
* How much does it advance our current knowledge?

We also request _overall_ ratings and predictions ... of the credibility, importance, and usefulness of the work, and to help _benchmark_ these evaluations to each other and to the current 'journal tier' system.

However, even here, the Unjournal metrics are also 'precise' in a sense that 'journal publication tiers' are not. There is no agreed-upon metric of _exactly_ how journals rank (e.g., within economics' 'top-5' or 'top field journals'). More importantly, there is no clear measure of the relative quality and trustworthiness of the paper _within particular journals,_

In addition, there are issues of lobbying, career concerns, and timing, discussed elsewhere, which make the 'tiers' system less reliable. An outsider doesn't know, e.g.,

* Was a paper published in a top journal because of a special relationship and connections, or an editor who was trying to push a particular agenda?
* Or was it published in a lower-ranked journal because the author needed to quickly get some points to fill their CV for an upcoming tenure decision?

In contrast, The Unjournal requires evaluators to give specific, precise quantified ratings and predictions (along with an explicit metric of their uncertainty over these).

Of course, our systems will not solve _all_ problems associated with reviews/evaluations: power dynamics, human weaknesses, and limited resources will remain. But we hope our approach moves in the right direction.

## Better feedback

See also [mapping-evaluation-workflow.md](../../our-policies-evaluation-and-workflow/mapping-evaluation-workflow.md "mention")

## Faster (public) evaluation

We want to reduce the time between 'when research is done' (and a paper or other research format) is released and 'when other people (academics, policymakers, journalists, etc.) have a credible measure of 'how much to believe the results' and 'how useful this research is'.

Here's how The Unjournal can do this.

1. _Early evaluation:_ We will evaluate potentially-impactful research soon after it is released (as a working paper, preprint, etc.) We will encourage authors to submit their work for our evaluation, and we will [directly commission](../../policies-projects-evaluation-workflow/considering-projects/direct-evaluation-track.md) the evaluation of work from the highest-prestige authors.
2. _We will pay evaluators_, with further incentives for timeliness (as well as carefulness, thoroughness, communication, and insight). [Evidence suggests](https://www.aeaweb.org/articles?id=10.1257/jep.28.3.169) that these incentives for promptness are likely to work.
3. _Public evaluations and ratings_: Rather than waiting years to see 'what tier journal a paper lands in', the public can simply consult The Unjournal to find credible evaluations and ratings.
4. See [#why-should-researchers-and-groups-submit-their-work-to-and-engage-with-the-unjournal](../../faq-interaction/for-researchers-authors/#why-should-researchers-and-groups-submit-their-work-to-and-engage-with-the-unjournal "mention")

[^1]: Subject to some potential temply embargos and rare exceptions for sensitive early-career researchers. We discuss these elsewhere. As of June 2023 there have been no such embargos or exceptions.

# Promoting open, robust science

## TLDR: Unjournal promotes research replicability/robustness

UJ evaluations aim to support the ‘Reproducibility/Robustness-checking’ (‘RRC’) agenda; we are directly engaging with the [Institute for Replication](https://i4replication.org/) (I4R) and [The repliCATS project](https://replicats.research.unimelb.edu.au/) (RC), and building connections to [Replication Lab](https://www.vilhuber.com/lars/projects/replication-lab/)/TRELiSS, and [Metaculus](https://www.metaculus.com/about/).

_We will support this agenda by:_

1. Asking pre-print authors to share code and data, evaluating and rewarding this
2. [dynamic-documents-vs-living-projects.md](dynamic-documents-vs-living-projects.md "mention")
3. Asking Unjournal evaluators to
   * Highlight the key/most relevant research claims/results/tests
   * Suggesting possible robustness checks/tests (RRC work)
   * Make predictions for these tests
4. Work with I4R and others to implement and evaluate computational replication and robustness-checking
5. Making the evaluation process open

## Research credibility

While the [replication crisis](https://www.wikiwand.com/en/Replication\_crisis) in psychology is well known, economics is not immune. Some very prominent and influential work has [blatant errors](https://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646), depends on [dubious econometric choices/faulty data,](https://www.nber.org/papers/w14130) is not[ robust to simple checks](https://economistsview.typepad.com/economistsview/2008/08/troubling-timin.html), or uses [likely-fraudulent data](https://www.science.org/content/article/fraudulent-data-set-raise-questions-about-superstar-honesty-researcher). Roughly 40% of experimental economics work [failed to replicate](https://www.science.org/content/article/about-40-economics-experiments-fail-replication-survey). Prominent commenters have [argued](https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review) that the traditional journal peer-review system does a poor job of spotting major errors and identifying robust work.

## UJ evaluation can help 'RRC'

My experience as a participant in the [SCORE replication market project](https://replicationmarkets.com/) made a key challenge clear to me (see [Twitter](https://twitter.com/search?q=%40ReplicationMkts%20%40givingtools\&src=typed\_query) posts): the value of the ‘replication’ depends on ‘which claims are chosen to reproduce, and how’. I often found the claim chosen to be replicated was not the most interesting or meaningful claim; or focused on a statistical result that was likely to narrowly reproduce but not imply what the authors asserted. Often the paper’s approach seemed flawed (e.g., not causally identified, or involving an experiment with important confounding factors) but I thought ‘if they do this again, they will probably get the same results’. And this came from a quick skim (evaluating hundreds of papers/claims). I think a detailed reading/analysis would do well at spotting the most important claims and explaining the needed RRC work. Indeed, these sorts of suggestions regularly come out of detailed, high-quality referee reports for economics journals (but these suggestions are often discarded, and they are rarely made public.)

At UJ, moving past our pilot round, we aim to ask evaluators to help highlight key ‘claims to replicate’, and replication goals and approaches. We will flag papers that particularly need replication in particular areas. Public evaluation and author responses inform this further, giving future replicators more than just the ‘published paper’ to work with. Obviously, this is most relevant where the papers we evaluate are published in journals I4Rep targets; our current focus on NBER papers makes overlap more likely.

**UJ gets authors to help with replication:** our platform and metrics encourage dynamic documents and transparency, which makes reproduction and replication easier. Prioritizing replicability and transparency at the working-paper stage (UJ evaluations’ current focus) makes authors more prepared to enable replication work later (e.g., after traditional publication)

**Robustness-reproduction Replication helps The Unjournal**

We aim to ask UJ evaluators to make predictions about replicability. To the extent these are replicated (with some probability), we can reward these. (The same holds for repliCATS aggregated/IDEA group evaluations: to know if we are credibly assessing replicability, we need to compare these at least some ‘replication outcomes’.) (Side benefit: the potential to 'inform replication efforts' may intrinsically motivate people to be UJ evaluators.)

<details>

<summary>Other mutual benefits/Synergies</summary>

We can rely on, and build a shared talent pool: UJ evaluators may be well-suited (and keen) to become robustness-reproducers (of the same, or other papers), as well as repliCATS participant.

We see the potential for synergy and economies of scale and scope in other areas, e.g., possible sharing of:

* IT/UX tools for capturing evaluator/replicator outcomes, statistical/info.-theoretic tools for aggregating these
* Protocols for data, code, instrument availability (e.g., [Data and Code Availability Standard](https://datacodestandard.org/))
* Communicating the synthesis of 'evaluation and replication reports'
* Encouraging institutions, journals, funders, and working paper series to encourage/require engagement

**More ambitiously, we may jointly interface with prediction markets.** We may also jointly integrate into platforms like OSF, as part of an ongoing process of preregistration, research, evaluation, replication, and synthesis.

</details>

<details>

<summary>Broader synergies in the medium term</summary>

As a 'Journal-independent evaluation' gains career value, as replication becomes more normalized, as we scale up ...

* This changes incentive systems for academics, which makes rewarding replication/replicability easier than with the traditional journals’ ‘accept/reject, start again elsewhere’ system.
* The Unjournal could also evaluate I4rep replications, giving them status
* Public communication of Unjournal evaluations and responses may encourage demand for replication work\\

In a general sense, we see cultural spillovers in 'willingness to try new systems for reward and credibility', and for the gatekeepers to reward this and not just the traditional 'publication outcomes'.

</details>

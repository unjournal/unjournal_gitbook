---
description: 'TLDR: Unjournal promotes research replicability/robustness'
---

# Promoting open and robust science

_Unjournal_ evaluations aim to support the "Reproducibility/Robustness-Checking" (RRC) agenda. We are directly engaging with the [Institute for Replication](https://i4replication.org/) (I4R) and [the repliCATS project](https://replicats.research.unimelb.edu.au/) (RC), and building connections to [Replication Lab](https://www.vilhuber.com/lars/projects/replication-lab/)/TRELiSS and [Metaculus](https://www.metaculus.com/about/).

We will support this agenda by:

1. **Promoting data and code sharing:** We request pre-print authors to share their code and data, and reward them for their transparency.
2. [dynamic-documents-vs-living-projects.md](dynamic-documents-vs-living-projects.md "mention"): Breaking out of "PDF prisons" to achieve increased transparency.
3. **Encouraging detailed evaluations:** Unjournal evaluators are asked to:
   * highlight the key/most relevant research claims, results, and tests;
   * propose possible robustness checks and tests (RRC work); and
   * make predictions for these tests.
4. **Implementing computational replication and robustness checking:** We aim to work with I4R and other organizations to facilitate and evaluate computational replication and robustness checking.
5. **Advocating for open evaluation:** We prioritize making the evaluation process transparent and accessible for all.

## Research credibility

While the [replication crisis](https://www.wikiwand.com/en/Replication\_crisis) in psychology is well known, economics is not immune. Some very prominent and influential work has [blatant errors](https://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646), depends on [dubious econometric choices or faulty data,](https://www.nber.org/papers/w14130) is not[ robust to simple checks](https://economistsview.typepad.com/economistsview/2008/08/troubling-timin.html), or uses [likely-fraudulent data](https://www.science.org/content/article/fraudulent-data-set-raise-questions-about-superstar-honesty-researcher). Roughly 40% of experimental economics work [fail to replicate](https://www.science.org/content/article/about-40-economics-experiments-fail-replication-survey). Prominent commenters have [argued](https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review) that the traditional journal peer-review system does a poor job of spotting major errors and identifying robust work.

## Supporting the RRC agenda through _Unjournal_ evaluations

My involvement with the [SCORE replication market project](https://replicationmarkets.com/) shed light on a key challenge (see [Twitter](https://twitter.com/search?q=%40ReplicationMkts%20%40givingtools\&src=typed\_query) posts): The effectiveness of replication depends on the claims chosen for reproduction and how they are approached. I observed that it was common for the chosen claim to miss the essence of the paper, or to focus on a statistical result that, while likely to reproduce, didn't truly convey the author's message.

Simultaneously, I noticed that many papers had methodological flaws (for instance, lack of causal identification or the presence of important confounding factors in experiments). But I thought that these studies, if repeated, would likely yield similar results. These insights emerged from only a quick review of hundreds of papers and claims. This indicates that a more thorough reading and analysis could potentially identify the most impactful claims and elucidate the necessary RRC work.

Indeed, detailed, high-quality referee reports for economics journals frequently contain such suggestions. However, these valuable insights are often overlooked and rarely shared publicly. _Unjournal_ aims to change this paradigm by focusing on three main strategies:

1. **Identifying vital claims for replication:**
   * We plan to have _Unjournal_ evaluators help highlight key "claims to replicate," along with proposing replication goals and methodologies. We will flag papers that particularly need replication in specific areas.
   * Public evaluation and author responses will provide additional insight, giving future replicators more than just the original published paper to work with.
2. **Encouraging author-assisted replication:**
   * _The Unjournal's_ platform and metrics, promoting dynamic documents and transparency, simplify the process of reproduction and replication.
   * By emphasizing replicability and transparency at the working-paper stage (_Unjournal_ evaluations’ current focus), we make authors more amenable to facilitate replication work in later stages, such as post-traditional publication.
3. **Predicting replicability and recognizing success:**
   * We aim to ask _Unjournal_ evaluators to make predictions about replicability. When these are successfully replicated, we can offer recognition. The same holds for repliCATS aggregated/IDEA group evaluations: To know if we are credibly assessing replicability, we need to compare these to at least some "replication outcomes."
   * The potential to compare these predictions to actual replication outcomes allows us to assess the credibility of our replicability evaluations. It may also motivate individuals to become _Unjournal_ evaluators, attracted by the possibility of influencing replication efforts.

By concentrating on NBER papers, we increase the likelihood of overlap with journals targeted by the Institute for Replication, thus enhancing the utility of our evaluations in aiding replication efforts.

<details>

<summary>Other mutual benefits/synergies</summary>

We can rely on and build a shared talent pool: _UJ_ evaluators may be well-suited—and keen—to become robustness-reproducers (of these or other papers) as well as repliCATS participants.

We see the potential for synergy and economies of scale and scope in other areas, e.g., through:

* sharing of IT/UX tools for capturing evaluator/replicator outcomes, and statistical or info.-theoretic tools for aggregating these outcomes;
* sharing of protocols for data, code, and instrument availability (e.g., [Data and Code Availability Standard](https://datacodestandard.org/));
* communicating the synthesis of "evaluation and replication reports"; or
* encouraging institutions, journals, funders, and working paper series to encourage or require engagement.

**More ambitiously, we may jointly interface with prediction markets.** We may also jointly integrate into platforms like OSF as part of an ongoing process of preregistration, research, evaluation, replication, and synthesis.

</details>

<details>

<summary>Broader synergies in the medium term</summary>

As a "journal-independent evaluation" gains career value, as replication becomes more normalized, and as we scale up:

* This changes incentive systems for academics, which makes rewarding replication/replicability easier than with the traditional journals’ system of "accept/reject, then start again elsewhere."
* _The Unjournal_ could also evaluate I4rep replications, giving them status.
* Public communication of _Unjournal_ evaluations and responses may encourage demand for replication work.

In a general sense, we see cultural spillovers in the willingness to try new systems for reward and credibility, and for the gatekeepers to reward this behavior and not just the traditional "publication outcomes".

</details>

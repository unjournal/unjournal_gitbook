---
description: 'TLDR: Unjournal promotes research replicability/robustness'
---

# Promoting open and robust science

Unjournal evaluations aim to support the ‘Reproducibility/Robustness-Checking’ (RRC) agenda. We are directly engaging with the [Institute for Replication](https://i4replication.org/) (I4R) and [the repliCATS project](https://replicats.research.unimelb.edu.au/) (RC), and building connections to [Replication Lab](https://www.vilhuber.com/lars/projects/replication-lab/)/TRELiSS, and [Metaculus](https://www.metaculus.com/about/).

We will support this agenda by:

1. **Promoting Data and Code Sharing:** We request pre-print authors to share their code and data, and reward them for their transparency.
2. [dynamic-documents-vs-living-projects.md](dynamic-documents-vs-living-projects.md "mention"): Breaking out of 'PDF prisons' for increased transparency.
3. **Encouraging Detailed Evaluations:** Unjournal evaluators are asked to:
   * Highlight the key/most relevant research claims/results/tests
   * Propose possible robustness checks/tests (RRC work)
   * Make predictions for these tests
4. **Implementing Computational Replication and Robustness-Checking:** We aim to work with I4R and other organizations to facilitate and evaluate computational replication and robustness-checking.
5. **Advocating for Open Evaluation:** We prioritize making the evaluation process transparent and accessible for all.

## Research credibility

While the [replication crisis](https://www.wikiwand.com/en/Replication\_crisis) in psychology is well known, economics is not immune. Some very prominent and influential work has [blatant errors](https://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646), depends on [dubious econometric choices/faulty data,](https://www.nber.org/papers/w14130) is not[ robust to simple checks](https://economistsview.typepad.com/economistsview/2008/08/troubling-timin.html), or uses [likely-fraudulent data](https://www.science.org/content/article/fraudulent-data-set-raise-questions-about-superstar-honesty-researcher). Roughly 40% of experimental economics work [failed to replicate](https://www.science.org/content/article/about-40-economics-experiments-fail-replication-survey). Prominent commenters have [argued](https://experimentalhistory.substack.com/p/the-rise-and-fall-of-peer-review) that the traditional journal peer-review system does a poor job of spotting major errors and identifying robust work.

## Supporting the RRC agenda through Unjournal evaluations

My involvement with the [SCORE replication market project](https://replicationmarkets.com/) shed light on a key challenge (see [Twitter](https://twitter.com/search?q=%40ReplicationMkts%20%40givingtools\&src=typed\_query) posts) - the effectiveness of replication depends on the claims chosen for reproduction, and how they are approached. It was common for the chosen claim to miss the essence of the paper, or to focus on a statistical result that, while likely to reproduce, didn't truly convey the author's message.

Simultaneously, I noticed that many papers had methodological flaws (for instance, lack of causal identification or the presence of important confounding factors in experiments). But I thought that these studies, if repeated, would likely yield similar results. These insights emerged from only a quick review of hundreds of papers and claims. This indicates that a more thorough reading and analysis could potentially identify the most impactful claims and elucidate the necessary RRC work.

Indeed, detailed, high-quality referee reports for economics journals frequently contain such suggestions. However, these valuable insights are often overlooked and rarely shared publicly. Unjournal aims to change this paradigm by focusing on three main strategies:

1. **Identifying Vital Claims for Replication:**
   * We plan to have Unjournal evaluators help highlight key 'claims to replicate', along with proposing replication goals and methodologies. We will flag papers that particularly need replication in particular areas.
   * Public evaluation and author responses will provide additional insight, giving future replicators more than just the original published paper to work with.
2. **Encouraging Author-Assisted Replication:**
   * Unjournal's platform and metrics, promoting dynamic documents and transparency, simplify the process of reproduction and replication.
   * By emphasizing replicability and transparency at the working-paper stage (Unjournal evaluations’ current focus), we make authors more amenable to facilitate replication work in later stages, such as post-traditional publication.
3. **Predicting Replicability and Recognizing Success:**
   * We aim to ask Unjournal evaluators to make predictions about replicability. When these are successfully replicated, we can offer recognition. The same holds for repliCATS aggregated/IDEA group evaluations: to know if we are credibly assessing replicability, we need to compare these at least some ‘replication outcomes’.
   * The potential to compare these predictions to actual 'replication outcomes' allows us to assess our replicability evaluations' credibility. It may also motivate individuals to become Unjournal evaluators, attracted by the possibility of influencing replication efforts.

By concentrating on NBER papers, we increase the likelihood of overlap with journals targeted by the Institute for Replication, thus enhancing the utility of our evaluations in aiding replication efforts.

<details>

<summary>Other mutual benefits/synergies</summary>

We can rely on, and build a shared talent pool: UJ evaluators may be well-suited (and keen) to become robustness-reproducers (of the same, or other papers), as well as repliCATS participant.

We see the potential for synergy and economies of scale and scope in other areas, e.g., possible sharing of:

* IT/UX tools for capturing evaluator/replicator outcomes, statistical/info.-theoretic tools for aggregating these
* Protocols for data, code, instrument availability (e.g., [Data and Code Availability Standard](https://datacodestandard.org/))
* Communicating the synthesis of 'evaluation and replication reports'
* Encouraging institutions, journals, funders, and working paper series to encourage/require engagement

**More ambitiously, we may jointly interface with prediction markets.** We may also jointly integrate into platforms like OSF, as part of an ongoing process of preregistration, research, evaluation, replication, and synthesis.

</details>

<details>

<summary>Broader synergies in the medium term</summary>

As a 'Journal-independent evaluation' gains career value, as replication becomes more normalized, as we scale up ...

* This changes incentive systems for academics, which makes rewarding replication/replicability easier than with the traditional journals’ ‘accept/reject, start again elsewhere’ system.
* The Unjournal could also evaluate I4rep replications, giving them status
* Public communication of Unjournal evaluations and responses may encourage demand for replication work\\

In a general sense, we see cultural spillovers in 'willingness to try new systems for reward and credibility', and for the gatekeepers to reward this and not just the traditional 'publication outcomes'.

</details>

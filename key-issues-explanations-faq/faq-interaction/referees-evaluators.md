# Evaluation (refereeing)

{% hint style="info" %}
We refer to 'evaluation' because the Unjournal does not _publish_ work; it only links, rates, and evaluates it. However, below, we will mainly refer to traditional terms like 'referees', indicating the same thing. We do this to make it more readable and familiar.
{% endhint %}

## Choosing, working with evaluators (referees)

### How do we choose 'referees'?

* Standard procedures (expertise, interest, lack of COI)
* Consult opt-in referee pool first

### Why pay referees?

* It is equitable, especially for those not getting 'service credit'
* While researchers currently write reports for prominent traditional journals for free (perhaps in exchange for goodwill when they submit their own work) we need to use explicit incentives as the Unjournal grows&#x20;
* We can use these as incentives for high quality work, especially if and when we increase the rate
* We can use patments to access wider range of expertise, including people not interested in submitting their own work to the Unjournal

## Public evaluations

### Why _public_ evaluations

#### Exceptions for ECRs?

### How are these hosted and shared?

* [Kotahi](https://kotahi.community/), linked to a [Sciety](https://sciety.org/) group (current plan)
* Also hosted/mirrored on our own page (once we set this up)
* Reviews will have a DOI, linked into all relevant systems

## Evaluation guidelines and criteria

### General criteria/guidelines

See [guidelines-for-evaluators.md](../../policies-projects-evaluation-workflow/policies-evaluation/guidelines-for-evaluators.md "mention") for our proposed approach and template for referees.&#x20;

_30 Jul 2022: This is a first-draft approach; many issues are still under discussion._



(See also: [#considering-communicating-editors-process](../../policies-projects-evaluation-workflow/policies-and-templates/considering-projects.md#considering-communicating-editors-process "mention"))

## Anonymity/blinding vs. signed reports

### Feedback and discussion vs. evaluations

> DR: I suspect that signed reviews (cf blog posts) provide good feedback and evaluation. However, when it comes to rating (quantitative measures of a paper's value), my impression from existing initiatives and conversations is that people are reluctant to award anything less than 5/5 'full  marks'.

### Why Single-blind?

* Power dynamics: referees don't want to be 'punished', may want to flatter powerful authors
* Connections and friendships may inhibit honesty
* 'Powerful referees signing critical reports' could hurt ECRs&#x20;

### Why signed reports?

* Public reputation incentive for referees
  * (But note single-blind paid review has some private incentives.)&#x20;
* Fosters better public dialogue
* Inhibits obviously unfair and impolite 'trashing'

### Compromise approaches

* Author and/or referee choose whether it should be single-blind or signed
* Random trial: We can compare empirically (are signed reviews less informative?)
* Use a mix (1 signed, 2 anonymous reviews) for each paper



## &#x20;

# Guideline for evaluators

{% hint style="info" %}
_**Thanks for considering evaluating research for the Unjournal!**_
{% endhint %}

## Here is what we would like you to do

1. _**Give quantitative metrics and predictions requested in the two tables below**_, as appropriate.
2. _**Write a review**_**, approximately a ‘typical referee report’, with some specific considerations**
3. _**Answer a short questionnaire**_

**In writing your review, please consider...**

<details>

<summary>Specific requests for focus/feedback</summary>

Anything our managers/editors specifically asked you to focus on. We may ask you to focus on specific areas of expertise; you do not need to address all aspects of the work. We may forward specific feedback requests from authors.&#x20;



</details>

<details>

<summary>The Unjournal's criteria</summary>

See [#category-metrics-multiple-dimensions](guideline-for-evaluators.md#category-metrics-multiple-dimensions "mention") for some guidance), e.g., prioritizing impact and robustness over cleverness.&#x20;

</details>

<details>

<summary><strong>Remember:</strong> This review will be made public*</summary>

... unless you were advised otherwise. It will be given a DOI, and hopefully enter the public research conversation. You will be given a _choice_ of whether you want to be publicly listed as an author of the review. &#x20;

</details>

_If you have questions or clarifications about the authors’ work, you can ask them these questions anonymously; we will facilitate it._

<details>

<summary>Notes </summary>

We are considering the best policy towards signed reviews vs. single-blind reports; we may change this policy in the future.

* We give early-career researchers the right to veto publication of very negative reviews, but we will inform you in advance if this is the case.&#x20;

<!---->

* You can reserve some ‘sensitive’ content in your report to share only with Unjournal management or only to the authors; but we hope to keep this limited.

</details>

{% hint style="info" %}
_Suggestion:_ The [#category-metrics](guideline-for-evaluators.md#category-metrics "mention") and [#overall-metrics-holistic-assessment](guideline-for-evaluators.md#overall-metrics-holistic-assessment "mention")outline our evaluation priorities. You may want to look at these metrics before writing your review, and then return to them after.
{% endhint %}

## Metrics: Overall assessment, categories

{% hint style="info" %}
_Below:_ a 'completed example' ... We will provide evaluators a concise 'survey form with everything to fill out'. In the form below we give _both_ 90% CIs and a confidence rating; evaluators only need to do one.

See further guidance and instruction in subsections below.&#x20;
{% endhint %}



<table><thead><tr><th>Category (importance)</th><th data-type="number">Rating (0-100)</th><th>90% CI</th><th data-type="rating" data-max="5">Confidence (alt.)</th></tr></thead><tbody><tr><td><strong>OVERALL ASSESSMENT (holistic, most important!)</strong></td><td>44</td><td>39, 52</td><td>4</td></tr><tr><td><h4>Advancing knowledge and practice (5/5)</h4></td><td>50</td><td>47, 54</td><td>5</td></tr><tr><td><h4>Methods: Justification, reasonableness, validity, robustness (5/5)</h4></td><td>51</td><td><em>45, 55</em></td><td>4</td></tr><tr><td><h4>Logic &#x26; communication ...(4/5)</h4></td><td>20</td><td><em>10, 35</em></td><td>3</td></tr><tr><td><h4>Open, collaborative, replicable... (3/5) </h4></td><td>60</td><td><em>40, 70</em> </td><td>2</td></tr><tr><td><h4>Relevance to global priorities(0/5)</h4></td><td>30</td><td><em>21, 65</em></td><td>1</td></tr></tbody></table>

_**Notes:** Please give a 90% CI or a confidence score, but not both. All metrics are explained below under_ [#what-we-are-asking-you-to-rate-explanations](guideline-for-evaluators.md#what-we-are-asking-you-to-rate-explanations "mention")_._



_**For each question above, if it seems relevant, and you feel qualified to judge, please ...**_

1. _G_ive a rating from 0-100, considering the _‘what we are asking you to rate’_ discussion provided. Quantify this following the [#0-100-metric-described](guideline-for-evaluators.md#0-100-metric-described "mention") scale but specifically for this _category._
2. (Optional) ‘Quantify how certain you are’ about this, either giving a 90% confidence interval, or using our [scale as described below](guideline-for-evaluators.md#the-confidence-rating-explained).

For each category, we give an ‘importance scale’ to suggest the importance of your rating for your overall assessment, considering the Unjournal priorities.&#x20;

{% hint style="info" %}
We see the 'overall assessment' as the most important measure. Please prioritize this.
{% endhint %}

### Overall assessment _- discussion_

Judge the work’s quality heuristically. Consider all aspects of quality, importance to knowledge production, and importance to practice, particularly including the category metrics reported above (if you like, weighted by the importance scores we gave).



### 0-100 Metric described

_The description below focuses on the "Overall Assessment" ... please try to use a similar  scale when evaluating the category metrics._

**95-100:** Among the highest quality and most important work you have ever read.

**90-100:** This work represents a major achievement, making substantial contributions to the field and practice. Such work would/should be weighed very heavily by tenure and promotion committees, and grant makers.

<details>

<summary>For example: </summary>

E.g., either

* Most work in this area in the next ten years will be influenced by this paper
* This paper is substantially more rigorous or more insightful than existing work in this area in a way that matters for research and practice
* The work makes a major, perhaps decisive contribution to a case for (or against) a policy or philanthropic intervention\


</details>

**75-90:** This work represents a strong and substantial achievement. It is highly rigorous, relevant, and well-communicated, up to the standards of the strongest work in this area (say, the standards of the top 5% of committed researchers in this field). Such work would/should not be decisive in a tenure/promotion/grant decision alone, but it should make a very solid contribution to such a case.

**60-75:** A very strong, solid, and relevant piece of work. It may have minor flaws or limitations, but overall it is very high-quality, meeting the standards of well-respected research professionals in this field.

**40-60:** A useful contribution, with major strengths, but also some important flaws or limitations.

**20-40:** Some interesting and useful points and some reasonable approaches, but only marginally so. Important flaws and limitations. Would need substantial refocus or changes of direction and/or methods in order to be a useful part of the research and policy discussion.

**0-20:** Among the lowest quality papers.&#x20;

<details>

<summary>"Lowest quality"...</summary>

Not making any substantial contribution and containing fatal flaws. This may be fundamentally addressing, or an issue that is not defined or obviously not relevant, or be substantially outside of the authors’ field of expertise.

</details>

**0-5:** Illegible, fraudulent, or plagiarized. _Please alert and flag fraud to us and to relevant authorities._

### The confidence rating ...

<details>

<summary>Why/what are we looking for?</summary>

In considering how to weigh any measure or evaluation, it is important to 'quantify the uncertainty'. That's why we ask you to give us a measure of this. You may feel comfortable giving your "90% confidence interval", or you may prefer to give a 'descriptive rating' of your confidence (from 'Extremely' to 'Not confident').  In the box below, we provide a suggested correspondence between these two measures.

</details>

<details>

<summary><em><strong>Confidence ratings vs. confidence intervals... a proposed scale</strong></em></summary>

**From 'five dots' to 'one dot'...**&#x20;

**5 = Extremely** confident, i.e., 90% confidence interval spans +/- 4 points or less)\*

**4 = Very** confident: 90% confidence interval +/- 8 points or less

**3 = Somewhat** confident: 90% confidence interval +/- 15 points or less&#x20;

**2 = Not very** confident: 90% confidence interval, +/- 25 points or less

**1 = Not** confident: (90% confidence interval +/- 25 points)

_If you prefer, you can simply give your 90% confidence/credible interval._

\*In at least one direction. Obviously if you rated it a 98 you cannot be under_-_estimating by more than 2 points, etc.



_See the diagram below_&#x20;

</details>

The diagram below gives an example (click to zoom); note we would like you to give a 90% CI _or_ a confidence rating (1-5 dots) but not both ... The example illustrates the proposed correspondence.

__![](<../../.gitbook/assets/image (3).png>)****

### **Categories: What we are asking you to rate (explanations)**

#### 1.  Advancing our knowledge and practice (5/5 importance)

Does the project make a contribution to the field or to practice, particularly in ways that will be relevant to our other criteria?

<details>

<summary>Less weight to ‘originality and cleverness’...</summary>

‘Originality and cleverness’ should be weighted less than the typical journal because The Unjournal focuses on **impact**. Papers that apply existing techniques and frameworks more rigorously than previous work and/or apply it to new areas, and in ways that provide **practical** insights for GP (global priorities) and interventions should be highly valued. More weight should be placed oncontribution to GP than to the academic field.

</details>

Do the insights generated inform our (‘posterior’) beliefs about important parameters and about the effectiveness of interventions?&#x20;

<details>

<summary>Note that we do not require a substantial <em>shift</em> in our expectations. </summary>

We do not require a substantial _shift_ in our expectations. For research that is conceptually and methodologically sound (including being adequately powered), carefully presented ‘null results’ can, in general, advance the field.

</details>

Does the project leverage and incorporate recent relevant and credible work in useful ways?

#### 2. Methods: Justification, reasonableness, validity, robustness (5/5 importance)

Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are all of the given results justified in the 'methods discussion'?&#x20;

Are the results/methods likely to be robust to reasonable changes in the underlying assumptions? Does the author demonstrate this (e.g., at least with a reasonable range of robustness checks, at best ‘mapping the space’ of possible reasonable specifications)?

Avoiding bias and questionable research practices (QRP)... Did the authors take steps to reduce bias from opportunistic reporting and QRP? For example, pre-registration, multiple hypothesis testing corrections, reporting flexible specifications.

<details>

<summary><em>Notes</em></summary>

We use the term “methods” here broadly; this may include choice/collection of data, experiment or survey design, statistical analysis, and simulation, among others.

</details>

#### 3. Logic and communication: Coherent and clear argument, communication, reasoning transparency (4/5 importance)

Are the goals/questions of the paper clearly expressed? Are concepts clearly defined/referenced?

Is the reasoning ‘transparent’ (on reasoning transparency, see, e.g., [Open Philanthropy's guide](https://www.openphilanthropy.org/research/reasoning-transparency/))? Are all of the assumptions and logical steps made clear? Does the logic of the arguments make sense? Is the argument written well enough to make it easy to follow?

Is the data and/or analysis presented relevant to the arguments made? Are the stated conclusions/results consistent with the evidence (or theoretical results/proofs) presented? Are the tables/graphs/diagrams easy enough to understand in the context of the narrative? (eg no errors in labeling

#### 4. Open, collaborative, replicable science and methods (3/5 importance)

**Replicability, reproducibility, data integrity:**

Would another researcher be able to perform the same analysis and get the same results? Are the method and its details explained sufficiently, in a way that would enable easy and credible replication? For example, a full description of analysis, code and software provided, and statistical tests fully explained. Is the source of the data clear?

Is the necessary data made available to others, to the extent reasonably possible and as applicable? Ideally, the cleaned data should also be clearly labeled and explained/legible. \*Are we likely to be able to construct the output from the shared code (and data)?

<details>

<summary><em>Note:</em> evaluators are not required to run/evaluate the code; this is at your discretion</summary>

...However, having a quick look at some of the elements could be helpful. Ideally, the author should give code that allows easy, full replication, e.g., a single R script that runs and creates everything, starting from the original data source, and including data cleaning files. This would make it fairly easy for an evaluator to check. E.g., see [this ‘levels of computational reproducibility’ ](https://bitss.github.io/ACRE/assessment.html#score)taxonomy.

</details>

**Consistency**:

Do the numbers in the paper (and code output, if checked) make sense? Are they internally consistent throughout the paper?&#x20;

{% hint style="info" %}
_Note_: errors and issues such as these will ideally be reported immediately to the authors, allowing them a chance to make a correction!
{% endhint %}

**Useful building blocks:**&#x20;

Do the authors provide tools, resources, data, and outputs that are likely to enable and enhance future work and meta-analysis?

#### 5. Engaging with real-world, impact quantification; practice, realism and relevance (2/5 importance)

Does the paper consider the real-world relevance of the arguments and results presented, perhaps engaging policy and implementation questions?

Is the setup particularly well-informed by real-world norms and practices? **** “Is this realistic, does it make sense in the real world?”&#x20;

(\*) Do the authors communicate their work in ways policymakers and decision- makers are likely to understand (perhaps in a supplemental ‘non-technical abstract’), without being misleading and oversimplifying?&#x20;

(\*) Do the authors present practical ‘impact quantifications’ such as cost-effectiveness analyses, or provide results enabling these?

{% hint style="info" %}
(\*) Note: The last 2 items should be seen as optional; the authors might be invited to additionally include this; in future we may be able to pay them to do this, if grant funding permits.
{% endhint %}



#### 6. Relevance to global priorities\*(0/5: not part of overall evaluation)

_**Note:** The management team has already considered this work and evaluated it as relevant to global priorities, before passing it to evaluators. Nonetheless, we would like your opinion._

Is this topic, approach and discussion _potentially_ useful to global priorities research and interventions? E.g., Does it help us evaluate what to prioritize for interventions and policy, improve interventions and policy, or improve our research and knowledge capacity for these?



## Journal/Prediction metrics

We would like to benchmark our evaluations against 'how research is currently judged'. We want to provide a bridge between the current 'accept or reject' system and an evaluation-based system.  We want our evaluations to be taken seriously by universities and policymakers. Thus we are asking you for two predictions in the table below. \
&#x20;

<table><thead><tr><th>Prediction metric</th><th data-type="number">Prediction 'journal tier' 0.0-5.0</th><th>90% CI</th><th data-type="rating" data-max="5">Confidence (alt.)</th><th data-type="select"></th></tr></thead><tbody><tr><td><strong>What ‘quality journal’ </strong><em><strong>will</strong></em><strong> this be published in?</strong></td><td>null</td><td><em>lower, upper</em></td><td>1</td><td></td></tr><tr><td><strong>Overall assessment on ‘scale of journals'</strong> (tier it <em>should be published in; see below)</em></td><td>null</td><td><em>lower, upper</em></td><td>1</td><td></td></tr></tbody></table>

{% hint style="info" %}
To better understand what we are asking here, please consult the subsections below\
"Journal metrics", "What quality journal...", and "Overall assessment on ‘scale of journals'"
{% endhint %}

### Journal metrics

For the 'prediction’ questions above, we are asking for a ‘journal quality rating prediction’ from 0.0 to 5.0. You can specify up to 2 digits (e.g., “4.4” or “2.0”) We are using this metric as we suspect it is more familiar to academics.

**The metrics are:**

0/5: Marginally respectable/Little to no value. Not publishable in any journal with scrutiny or credible WP series, not likely to be cited by credible researcher

2/5: OK/Somewhat valuable journal

3/5: Top B-journal/Strong field journal

4/5: Marginal A-Journal/Top field journal

5/5: A-journal/Top Journal

We give some examples of actual journals [HERE](https://docs.google.com/spreadsheets/d/1nnS0FMOIKz-3rFJMn-JsX-6\_oE54gdvCjxs0vac6WF8/edit#gid=0) that might line up with these metrics based on SJR and the ABS ratings.

### What ‘quality level journal’ do you expect this work to be ultimately published in?

<details>

<summary>What if this work has already been 'peer-review published'?</summary>

The question above presumes that this work has not already been published in a peer-reviewed journal. However, we are planning to commission at least some post-publication review going forward. If the work already has been ‘peer-review-published’ you can either

* Skip this question or
* Answer a related question (not a prediction): “Suppose this paper were submitted to journals, in succession, from the top tier downwards. We made imagine there is some randomness in this process. Consider all possible “random draws of the world”. In the ‘median draw’, what ‘quality level journal’ would this paper be published in.

</details>

In presenting your prediction and confidence interval for this, you might want to consider if you were offering advice to an author:

* “What journal would be likely to publish this work?”
* “What is the most prestigious journal that would consider publishing this?”
* “What is the least prestigious journal that the authors should consider submitting this to? (I.e., I wouldn't go lower, even if I were risk-averse)”

<details>

<summary>Reprising the confidence intervals for this new metric</summary>

Dots given ...\
\
5: Extremely confident (90% confidence interval spans +/- 0.2 points or less)

4:  Very confident (90% confidence interval +/- 0.4 points or less)

3: Somewhat confident (90% confidence interval +/- .75 points or less)

2: Not very confident (90% confidence interval, +/- 1.25 points or less)

1: Not confident (90% confidence interval +/- more than 1.25 points)



</details>

### **Overall assessment on ‘scale of journals'**

Consider the scale of journals described above. Suppose that:

1\. the journal process was fair, unbiased and free of noise, and that status, social connections and ‘lobbying to get the paper published’ didn’t matter, and&#x20;

2\. journals assessed research according to the category metrics we discuss above.&#x20;

_In such a case, what ‘quality level journal’ would and should this research be published in its current form or with minor revisions?_

## Survey questions

{% hint style="info" %}
_For the questions below...  we will publish your responses along with your review unless you ask us to keep them anonymous._
{% endhint %}

1. How long have you been in this field?
2. How many proposals and papers have you evaluated?

_Your answers to the questions below will not be made public:_

1. How would you rate this template and process?
2. Do you have any suggestions or questions about this process or the Unjournal (we will try to respond, and incorporate your suggestions)? \[Open response]
3. Would you be willing to consider evaluating a revised version of this project?

## **How to write a good review (general conventional guidelines)**

<details>

<summary>Some general key points to consider</summary>

* Cite evidence and reference specific parts of the research when giving feedback.
* Try to justify your critiques and claims in a reasoning transparent way, rather than merely ‘passing judgment’.
* Provide specific, actionable feedback to the author where possible.
* When considering authors’ arguments, consider the most-reasonable interpretation of what they have written (and state what that is, to help the author make their point more clearly). See ‘[steelmanning](https://www.lesswrong.com/tag/steelmanning)’.
* Be collegial and encouraging, while being rigorous. Criticize and question specific parts of the research without suggesting criticism of the _researcher themself._

</details>

We are happy for you to use whichever process and structure you feel comfortable with when writing a peer review.&#x20;

<details>

<summary>One possible structure:</summary>

_Core_

* Overall score based on quantitative metrics, (possible: brief discussion of these metrics).
* Summarize the work and issues, and the research in context to convey your understanding and help others understand it.
* Positive aspects of the paper, strengths and contributions.
  * Assess the contribution of the work in context of existing research.
* Major limitations and potential ways the work could be improved; where possible, reference methodological literature and discussion, and work that ‘does what you are suggesting’.

_Optional_

* Discuss minor flaws and their potential revisions.
  * You are not obliged/paid to spend a great deal of time copy-editing the work. If you like, you can give a few specific suggestions and then suggest that the author look to make other changes along these lines.
* Suggestions for research agendas, for increasing the impact of the work, for incorporating the work into global priorities research and impact evaluations, and for enhancing future work.

</details>

{% hint style="info" %}
**Remember**: The Unjournal doesn’t “publish” and doesn’t “accept or reject”. So don’t give an “Accept, Revise and Resubmit, Reject, etc. ” recommendation. We just want quantitative metrics, some written feedback, and some relevant discussion.
{% endhint %}

### Some other resources/guides to writing reports

[The Wiley Online Library](https://authorservices.wiley.com/Reviewers/journal-reviewers/how-to-perform-a-peer-review/step-by-step-guide-to-reviewing-a-manuscript.html) (Conventional, general)

[PLOS](https://plos.org/resource/how-to-write-a-peer-review/) (Conventional but open access, simple and brief)

[Peer Community In... Questionnaire ](https://peercommunityin.org/2022/05/20/questionnaire-for-reviewers/)(Open-science-aligned)

[Open Reviewers Reviewer Guide ](https://zenodo.org/record/5484087)(Journal-independent “Pre-review”; detailed, targets ECRs)

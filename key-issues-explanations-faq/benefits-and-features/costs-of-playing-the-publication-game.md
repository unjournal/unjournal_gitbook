# Costs of 'the publication game'

[Link](https://docs.google.com/document/d/1GFISlF5TieCuA6jDYkYlNWaEpuEYrr\_zTmaVpTfBg4A/edit#heading=h.e1wqoks5tivx) to discussion space

## Rate and give feedback, don’t accept/reject

**Claim:** _Rating and feedback is better than an ‘all-or-nothing’ accept/reject process. Although people like to say “peer review is not binary”, the consequences are._

We use “publication in top journals” to measure two main things. First, policymakers (including important regulators like the FDA) and journalists use these to assess whether research is credible and reputable. Second, universities and other institutions use this to guide hiring, tenure, promotion, grants, and other ‘rewards for researchers.’

{% hint style="info" %}
**Did you know?:** More often than not, a[cademic economists speak of](https://twitter.com/search?q=%22Yes%20I%20always%20found%20it%20bizarre%20that%20we%20talk%20about%20the%20%E2%80%9Csupply%E2%80%9D%20of%20spots%20in%20coveted%20journals%20vs%20the%20%E2%80%9Cdemand%E2%80%9D%20for%20publishing%20there%22\&src=typed\_query) the "supply of spaces in journals” and the “demand to publish in these journals”. Who is the consumer? Certainly not the perhaps-mythical creature known as the ‘reader’.
{% endhint %}



### This can  be incredibly slow and risky for career academics

[In Economics it is not unusual](https://www.nber.org/papers/w29147) for it to take years between the ‘first working paper’ that is publicly circulated and presented  and the final publication. During that time, the paper may be substantially improved, but it may not be known of nor accepted by practitioners, and meanwhile the authors get little or no career value from it.&#x20;

_As a result…_&#x20;

__

1. **Researchers/Academics spend a tremendous amount of time "gaming" this process**.\[^5]&#x20;
2. It leads to a great deal of **randomness in outcomes,** leading the cream to not always rise to the top, making research careers much more stressful (driving out more risk-averse researchers).
3. A lot of **‘feedback’ is wasted** (and [reviewer’s time](https://www.aje.com/arc/peer-review-process-15-million-hours-lost-time/)) and a lot of effort is spent trying to overly curry favor with reviewers.

_But_ _do we need REJECTION_ as a hard filter to weed out flawed and messy content? Perhaps not: we are accustomed to using ratings as filters in our daily lives. Readers, grantmakers, and policymakers can decide for themselves what ‘floor’ they want to use, discarding papers and project that have (e.g.) 2+ peer reviews with an average accuracy rating above 3 and an average impact rating above 4.

### Researchers/Academics spend a tremendous amount of time "gaming" this process.

I (Reinstein) have been in Academia for about 20 years. Around the coffee pot, cafeteria table, research conferences, etc., roughly half of what we talked about was... the actual theory, methods, and results? No, we talk about “who got into which journal and how unfair it is”, “which journal should we be submitting our papers to?”, how long are their “turnaround times”, “how highly rated are these journals,” etc. We offer [tips](https://twitter.com/search?q=%22%20how%20to%20strategically%20please%20referees%20and%20sneak%20it%20into%20journals%22\&src=typed\_query) on how to [‘sneak into these journals’.](https://twitter.com/GivingTools/status/1188786422381268992)&#x20;

There is a lot of pressure, and even [bullying](http://bulliedintobadscience.org/), to achieve these “publication outcomes” at the expense of careful methodology.

<details>

<summary>Quotes</summary>

Paula Masuzzo: “I was told that publishing in Nature/Cell/Science was more important than everything else.”&#x20;

</details>

{% hint style="info" %}
**But do we need REJECTION as a hard filter to weed out flawed and messy content?**&#x20;

Perhaps not: we are accustomed to using ratings as filters in our daily lives. Readers, grantmakers, and policymakers can decide for themselves what ‘floor’ they want to use, discarding papers and project that have (e.g.) 2+ peer reviews with an average accuracy rating above 3 and an average impact rating above 4.
{% endhint %}

### It leads to a great deal of randomness in outcomes...

... leading the cream to not always rise to the top, making research careers much more stressful (driving out more risk-averse researchers).

<details>

<summary>Quote</summary>

This game takes away the creativity, the risk, the ‘right to fail’. This last item is for me, personally, very important and often underestimated. Science is mostly messy. Whoever tells us otherwise, is not talking about Science.”

</details>

### A lot of **‘feedback’ is wasted** (and [reviewer’s time](https://www.aje.com/arc/peer-review-process-15-million-hours-lost-time/))&#x20;

Some reviewers write ten-page reports critiquing the paper in great detail (to impress editors, or because they feel compelled to do so?), even when they reject the paper.  These are sometimes very informative and useful, and would also be very helpful for the wider public and research community to understand the nature of the debate and issues. However, researchers often have a very narrow focus on getting the paper published as quickly and in as high prestige a journal as possible. Unless the review is part of a 'Revise and Resubmit' that the author wants to fulfill, they may not actually put the comments into practice or address them in any way.&#x20;

Of course, the reviews maybe misinformed, mistaken, or misunderstanding. However,  if the paper is rejected (even if the reviewer themself was positive about the paper), the author has has no opportunity or incentive to respond to them. Thus the misinformed reviewer may remain in the dark.&#x20;

The other side of the coin:  a lot of effort is spent trying to overly curry favor with reviewers who are often seen as overly fussy and not always in the direction of good science.\



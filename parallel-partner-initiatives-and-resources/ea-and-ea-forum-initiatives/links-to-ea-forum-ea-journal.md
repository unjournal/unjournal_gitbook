---
description: >-
  This initiative and EA/gp 'unjournal' will interact with the EA forum  and
  build on initiatives coming there.
---

# Links to EA Forum/'EA journal'

Some of these links come _from a conversation with Aaron Gertler_

## EA Forum suggestions and formats

* Here's where to [suggest new Forum features](https://forum.effectivealtruism.org/posts/NhSBgYq55BFs7t2cA/ea-forum-feature-suggestion-thread)
* Here's an example of a [PR FAQ post](https://forum.effectivealtruism.org/posts/vLoKHGgyDom4Acine/pr-faq-sharing-readership-data-with-forum-authors) that led us to develop a new feature

Note: Reinstein and Hamish Huggard have worked on tools to help transform R-markdown and bookdown files. Some work  can be found on [this Repo](https://github.com/daaronr/dr-rstuff/tree/master/bookdown\_template/wip\_rmd\_to\_forum\_templating) (but it may need some explanation)



## Peer review \*on\* the EA forum?

Jaime Sevilla has thoughts on creating a peer review system for the Forum. (See embedded doc below, link [here](https://docs.google.com/document/d/1MmVhYPikZBRnNhKOdZ\_3qvfzpVb8YqmGb9Y9HhLFchQ/edit))

{% embed url="https://docs.google.com/document/d/1MmVhYPikZBRnNhKOdZ_3qvfzpVb8YqmGb9Y9HhLFchQ/edit" %}



### Peter Slattery

> 1. To create a quick and easy prototype to test you fork the EA forum and use that fork as a platform for the 'unjournal' project (maybe called something like 'The Journal of Social Impact Improvement and Assessment').
> 2. People (ideally many EA) would use the forum like interface to submit papers to this 'unjournal'.
> 3. These papers would look like EA forum posts but with an included OSF link to a PDF version. Any content (e.g., slides/video) could be embedded in the submission.
> 4. All submissions would be reviewed by a single admin (you?) for basic quality standards.
> 5. Most drafts would be accepted to the unjournal.
> 6. Any accepted drafts would be publicly 'peer reviewed'. They would achieve 'peer reviewed' status when >x (3?) people from a predetermined/elected editors/expert board had publicly or anonymously reviewed the paper by commenting publicly on the post. Reviews might also involve ratings the draft on relevant criteria (INT?). Public comment/review/rating would also be possible.&#x20;
> 7. Draft revisions would be optional but could be requested. These would simply be new posts with version X/v X appended to the title
> 8. All good comments/posts to the journal would receive upvotes etc so authors, editors and commentators would gain recognition, status and 'points' etc from participation. This is sufficient for generating participation in most forums and notably lacking in most academic settings.
> 9. Good papers submitted to the journal would be distinguished by being more widely read, engaged with, and praised than others. If viable, they would also win prizes. As an example, there might be a call for papers on solving issue x with a reward pool of grant/unconditional funding of up to x for winning submissions. The top x papers submitted to the unjournal in response to that call would get grant funding for further research.
> 10. A change in reward/incentives (from I had a paper accepted/cited to I won a prize), seem to have various benefits
> 11. It still works for traditional academic metrics - grant money is arguably even more prized than citations and publications in many settings
> 12. It works for non-academics who don't care about citations or prestigious journal publications
> 13. As a metric 'funds received' would probably better tracks researchers' actual impact than their citations and acceptance in a top journal. People won't pay for more research that they don't value but they will cite or accept that to a journal for other reasons.
> 14. Academics could of course still cite the DOIs and get citations tracked this way.
> 15. Reviewers could be paid per review by research commissioners.
> 16. Here is a quick example of how it could work for the first run. Open Philanthropy call for research on something they want to know about (e.g., interventions to reduce wild animal suffering). They commit to provide up 100,000 in research funding for good submissions and 10,000 for review support. 10 relevant experts apply and are elected to the expert editorial boards to review submissions. They will receive 300USD per review and are expected to review at least x paper.  People submit papers, these are reviewed, OP award follow up prizes to the winning papers. The cycle repeats with different funders and so on.
>
> I suppose I like the above because it seems pretty easy and actionable to do over as a test run for something to refine and scale. I estimate that I could probably do it myself if I had 6-12 months to focus on it. However, I imagine that I am missing a few key considerations as I am usually over-optimistic! Feel free to point those out and offer feedback.

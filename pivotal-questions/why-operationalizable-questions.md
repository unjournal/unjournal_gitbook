# Why "operationalizable questions"?

Why are we seeking these pivotal questions to be 'operationalizable'?\


1. This is in line with [our own focus](https://globalimpact.gitbook.io/the-unjournal-project-and-communication-space/policies-projects-evaluation-workflow/considering-projects/what-specific-areas-do-we-cover#fields-methods-and-approaches) on this type of research
   1. The Unjournal [mainly focuses on](#user-content-fn-1)[^1] evaluating (largely empirical) research that clearly poses and answers specific impactful questions, rather than research that seeks to define a question, survey a broad landscape of other research, open routes to further inquiry, etc.&#x20;
2. I think this will help us focus on fully-baked questions, where the answer is likely to provide actual value to the target organization and others (and avoid the old ‘[42’](https://www.wikiwand.com/simple/42\_\(answer\)) trap).
3. It offers potential for benchmarking and validation (e.g., using prediction markets), specific routes to measure our impact (updated beliefs, updated decisions), and informing the ['claim identification (and assessment)'](https://docs.google.com/document/d/1mBkAmCVomcUt0Ks7hsxShTsjAbx3WVtFfMCnasGQxns/edit) we’re asking from evaluators (see footnote above).

However, as this initiative progresses we may allow a wider range of questions, e.g., more open-ended, multi-outcome, non-empirical (perhaps ‘normative), and best-practice questions.

[^1]: However, we have evaluated some broader work where it seemed particularly high impact, original, and substantive.  E.g., we’ve evaluated work in ‘applied economic theory’ such as [Aghion et al](https://unjournal.pubpub.org/pub/aimetrics/release/9). on the impact of artificial intelligence on economic growth, and applied methodology, e.g., [ "Replicability & Generalisability: A Guide to CEA discounts"](https://unjournal.pubpub.org/pub/evalsumtemplateapplied/release/1).

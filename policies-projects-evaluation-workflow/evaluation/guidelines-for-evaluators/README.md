# Guidelines for evaluators

{% hint style="info" %}
_**Thanks for your interest in evaluating research for**_** The Unjournal!**

Your evaluation will be made public and given a DOI, but you have the option to remain anonymous or to "sign your review" and take credit.

[You will be given a (minimum) $400 honorarium](#user-content-fn-1)[^1] for providing an on-time[^2] and complete evaluation and feedback. You will also be eligible for monetary prizes for "most useful and informative evaluation," plus other bonuses.\
\
See the guidelines below. You can submit your response in a [Google Doc (see note](#user-content-fn-3)[^3])**,** and share it back with us. Click [here](https://docs.google.com/document/d/1erOQ8qiWmgAmd9WdMLmuBGoxFkUJeQo2c8pc5wFAQbk/copy) to make a new copy of this form directly.  \


Dec. 2023 update: we are moving to a hosted form/interface in PubPub; you may have been provided this link instead.\


Our data protection statement is linked [here](https://bit.ly/46y0LqH).
{% endhint %}

<details>

<summary>Managers/commenters</summary>

The 'template Google Doc itself [can be accessed here](https://docs.google.com/document/d/1erOQ8qiWmgAmd9WdMLmuBGoxFkUJeQo2c8pc5wFAQbk/edit#heading=h.lst8joh1kli3).' Please request access permission.

Dec. 2023 update: we are moving to a hosted form/interface in PubPub, which will replace that Google Doc.

</details>

_You can download the current page as a PDF_ [_here_ ](https://www.dropbox.com/scl/fi/gixso4ir43o3n6d5u92qp/The-Unjournal-Evaluation-Guidelines.pdf?rlkey=cdqdkci0cds61hsza0787c5je\&dl=0)_for easier viewing, with all folded boxes open_ \[updated 28 Nov 2023].

<details>

<summary>2 Oct. 2023: Updates and simplifications</summary>

We are in the process of building a more clearly justified and interpretable set of guidelines, leveraging insights and innovations from previous work (such as repliCATS). This will take some time. \
\
In the meantime we have made some updates and simplifications; these are noted below. The main changes made so far are:

1. Removing the "suggested weightings'" for ratings categories
2. Adjusting the discussion of the "overall assessment" category

</details>

<details>

<summary>Additional rewards and incentives</summary>

We may occasionally offer additional payments for specifically requested evaluation tasks, or raise the base payments for particularly hard-to-source expertise.

July 2023: The above is our current policy; we are working to build an effective, fair, transparent, and straightforward system of honorariums, incentives, and awards for evaluators.

Dec 2023: Note that we currently set aside an additional $150 per evaluation (i.e., per evaluator) for evaluator incentives, bonuses, and prizes.

</details>

<details>

<summary>Submitting claims and expenses</summary>

See: [#submitting-and-paying-expenses-claims](../../../management-tech-details-discussion/fiscal-hosting-and-expenses.md#submitting-and-paying-expenses-claims "mention")

</details>

## What we would like you to do

1. **Write a review:** a "standard high-quality referee report," with some [specific considerations](#user-content-fn-4)[^4].
2. [**Give quantitative metrics and predictions** as requested in the two tables below.](#user-content-fn-5)[^5]
3. Answer a short questionnaire about your background and our processes.

## Writing the report

**In writing your report (and providing ratings), please consider the following:**

<details>

<summary>Specific requests for focus or feedback</summary>

Please pay attention to anything our managers and editors specifically asked you to focus on. We may ask you to focus on specific areas of expertise; you do not need to address all aspects of the work. We may also forward specific feedback requests from authors.

</details>

<details>

<summary><em>The Unjournal</em>'s criteria</summary>

For the most part, this is like a standard journal review, but we have some particular priorities.&#x20;

See[#category-explanations-what-you-are-rating](./#category-explanations-what-you-are-rating "mention") for guidance. For example, we would like to prioritize impact and robustness over cleverness.

</details>

<details>

<summary>Note that <strong>t</strong>his review (and ratings) will be made public</summary>

Unless you were advised otherwise, it will be given a DOI and, hopefully, will enter the public research conversation. Note that the authors will be given two weeks to respond to reviews before the evaluations, ratings, and responses are made public. You will be given a _choice_ of whether you want to be listed publicly as an author of the review.

</details>

_If you have questions or clarifications about the authors’ work, you can ask them these questions anonymously; we will facilitate it._&#x20;

_We want you to evaluate the most recent/relevant version of the paper/project that you can access. If you see a more recent version than the one we shared with you, please let us know._&#x20;

<details>

<summary>Publishing and signing reviews: considerations and exceptions</summary>

We are still considering the best policy towards signed reviews vs. single-blind reports. For now we give evaluators the option to choose, and you can wait to choose until _after_ you have completed the report. We may change this policy in the future.&#x20;

We may give early-career researchers the right to veto the publication of very negative reviews or to embargo the release of these reviews for a defined period. We will inform you in advance if this will be the case for your evaluation.

You can reserve some "sensitive" content in your report to be shared with only _The Unjournal_ management or only the authors, but we hope to keep this limited.

</details>

{% hint style="info" %}
_Suggestion to evaluators:_&#x20;

The [Metrics: overall assessment, categories](./#metrics-overall-assessment-categories) and [Overall Assessment](./#overall-assessment) outline our suggested evaluation priorities. You may want to look at these metrics _before_ writing your evaluation. However, we are not asking you to _structure_ your evaluation text according to these metrics.
{% endhint %}

### **What should your evaluation prioritize?**

In doing your evaluation, you might consider: "who is the audience?",  "what value is this evaluation meant to provide?",  and "how important is it to give feedback"? Essentially, we aim for a balanced emphasis on three areas:

1. Crafting evaluations and ratings that are informative and beneficial for readers and policymakers.
2. Ensuring these evaluations are meaningful for "assessing academics" offering a valuable metric that can be considered alongside the existing "journal tier" system.
3. Providing constructive feedback to researchers, helping them improve their work.

_We provide a more detailed discussion of what this means and why, and how it relates to our impact and "theory of change"_ [_here_](../../../faq-interaction/referees-evaluators.md#the-value-of-these-evaluations-what-to-prioritize)_._

### A **"standard high-quality referee report"**

We are generally asking for the sort of report an academic would write for a traditional high-prestige journal. We are asking for this, subject to some differences in priorities (discussed below) and subject to any particular requests the managing editor may communicate to you.

**Length/time spent:** This is up to you. We welcome detail, elaboration, and technical discussion.

<details>

<summary>Length and time: possible benchmarks</summary>

[The Econometrics society](https://www.econometricsociety.org/publications/econometrica/browse/guidelines-referees) recommends a 2–3 page referee report; [Berk et al.](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.31.1.231) suggest this is relatively short, but confirm that brevity is desirable. [In a recent survey (Charness et al., 2022)](https://evalresearch.weebly.com/report---full-text.html), economists report spending (median and mean) about one day per report, with substantial shares reporting "half a day" and "two days." We expect that reviewers tend spend more time on papers for high-status journals, and when reviewing work that is closely tied to their own agenda.

</details>

## Metrics: overall assessment, categories

Our _general priorities_ are embodied in the quantitative metrics below. We believe these are similar, but not identical, to criteria used by the top journals in economics and adjacent fields.

{% hint style="info" %}
Below is a completed example. We give evaluators a concise survey form with everything they need to fill out.
{% endhint %}

<details>

<summary>Oct 2023 update - removed "weightings"</summary>

We have removed suggested weightings for each of these categories. We discuss the rationale at some length [here](why-these-guidelines.md#weightings-for-each-rating-category-removed-for-now).&#x20;

Evaluators working before October 2023 saw a previous version of the table, which you can see [HERE](why-these-guidelines.md#pre-october-2023-ratings-with-weights-table-provided-for-reference).

</details>

<details>

<summary>Dec. 2023: Hiding/de-emphasizing 'confidence Likerts'</summary>

We previously gave evaluators two options for expressing their confidence in each rating:&#x20;

Either:

1. The 90% Confidence/Credible Interval (CI) input you see below (now a 'slider' in PubPub V7) or

<!---->

2. A five-point 'Likert style' measure of confidence, which we described qualitatively and explained how we would convert it into CIs when we report aggregations.&#x20;

To make this process less confusing, to encourage careful quantification of uncertainty, and to enable better-justified aggregation of expert judgment, we are de-emphasizing the latter measure.&#x20;

Still, to accommodate those who may not be familiar with or comfortable stating "90% CIs on their own beliefs" we offer further explanations, and we are providing tools to help evaluators  construct these. As a fallback, we will still allow evaluators to give the 1-5 confidence measure, noting the correspondence to CIs, but we discourage this somewhat.&#x20;

The previous guidelines [can be seen here](why-these-guidelines.md#pre-2024-ratings-and-uncertainty-elicitation-provided-for-reference-no-longer-in-use); these may be useful in considering evaluations provided pre-2024.

</details>

<table><thead><tr><th width="392">Category (importance)</th><th width="146.01169590643275" align="center">Rating (0-100)</th><th width="307" align="center">90% CI</th><th data-hidden></th></tr></thead><tbody><tr><td><a data-mention href="./#overall-assessment">#overall-assessment</a>(holistic, most important!)</td><td align="center">44</td><td align="center">39, 52</td><td></td></tr><tr><td><a data-mention href="./#1.-advancing-our-knowledge-and-practice">#1.-advancing-our-knowledge-and-practice</a></td><td align="center">50</td><td align="center">47, 54</td><td></td></tr><tr><td><a data-mention href="./#2.-methods-justification-reasonableness-validity-robustness">#2.-methods-justification-reasonableness-validity-robustness</a></td><td align="center">51</td><td align="center"><em>45, 55</em></td><td></td></tr><tr><td><a data-mention href="./#3.-logic-and-communication">#3.-logic-and-communication</a></td><td align="center">20</td><td align="center"><em>10, 35</em></td><td></td></tr><tr><td><a data-mention href="./#4.-open-collaborative-replicable-science-and-methods">#4.-open-collaborative-replicable-science-and-methods</a></td><td align="center">60</td><td align="center"><em>40, 70</em></td><td></td></tr><tr><td><a data-mention href="./#5.-engaging-with-real-world-impact-quantification-practice-realism-and-relevance">#5.-engaging-with-real-world-impact-quantification-practice-realism-and-relevance</a></td><td align="center">35</td><td align="center"><em>30,46</em></td><td></td></tr><tr><td><a data-mention href="./#6.-relevance-to-global-priorities">#6.-relevance-to-global-priorities</a></td><td align="center">30</td><td align="center">21,65</td><td></td></tr></tbody></table>

_All metrics are explained below under_[#what-we-are-asking-you-to-rate-explanations](./#what-we-are-asking-you-to-rate-explanations "mention")

<details>

<summary><em>Note: Relevance to global priorities</em> </summary>

Although we ask you to rate (and discuss) the relevance of this work to global priorities, you may not want this to heavily enter into your overall assessment rating. Why not? While  "relevance to global priorities" is very important, we want the overall assessment here to be somewhat comparable to that of traditional journals, to enable benchmarking.

</details>

&#x20;                &#x20;

_**For each question above, if it seems relevant and you feel qualified to judge, please . . .**_&#x20;

1. Give a rating from 0–100, considering the _"what we are asking you to rate"_ discussion provided. Try to follow the scale in[#0-100-metric-explained](./#0-100-metric-explained "mention"), but specifically for this _category._
2. Quantify how certain you are about this rating by giving a 90% [confidence](https://en.wikipedia.org/wiki/Confidence\_interval)/[credibility](https://en.wikipedia.org/wiki/Credible\_interval) interval.

_**Note**_: We are in the process of switching to a new PubPub interface for entering  evaluations. This provides "slider bars" to express your chosen intervals, as in the image below. This form will ask you to "please indicate your rating (the ‘middle’ dot), and the lower and upper endpoints of your 90% CI (the lower and upper dots)".

<figure><img src="../../../.gitbook/assets/image (16).png" alt=""><figcaption><p>A "Slider bar" for a rating</p></figcaption></figure>



<details>

<summary>But what do these measures mean, and how should I consider the 0–100 metric?</summary>

We explain the [categories here](https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/policies-projects-evaluation-workflow/evaluation/guidelines-for-evaluators#category-explanations-what-you-are-rating) and the intended metrics scale [here (links below)](./#0-100-metric-explained). Your feedback is very welcome.

</details>

<details>

<summary>Why are you asking about "confidence" in these metrics?</summary>

We would like you to state your "confidence intervals" or "credibility intervals." Loosely speaking, we hope to capture a sense of how sure you are about your ratings. This will help people who read your evaluation to know how much weight to put on them in using them for making their own decisions. These can also be used in systematic ways for meta-science and meta-analysis. We can "aggregate expert judgment" to get a better measure of how confident we _should be_ about particular measures and claims.

</details>

<details>

<summary>But how do I actually come up with these ratings and confidence/credibility intervals?</summary>

You may know most of the concepts below, but you might be unfamiliar with applying them in a situation like this one.

Suppose your best guess for the "Methods..." criterion is 65. Still, even an expert can never be certain. E.g., you may misunderstand some aspect of the paper, there may be a justification or method you are not familiar with, you might not understand the criterion, etc.

Your "uncertainty" over this could be described by some distribution, representing your beliefs about the _true value_ of this criterion. By "true value," you might think, "If you had all the evidence, knowledge, and wisdom in the world, and the benefit of perfect hindsight, what value would you choose for this criterion?"

Your "'best guess" should (basically) be the central mass point of this distribution.

You are asked to give a 90% interval. Loosely speaking, you could consider something like, "What is the smallest interval around this best guess that I believe is 90% likely to contain the true value?"

_**E.g., you might have thoughts similar to these:**_

I am going to interpret the 'methods' in terms of their reliability for consistent causal inference and minimizing parameter mean-squared error in settings like this one."

I will consider the appropriateness of the methods chosen relative to the choices made across the distribution of all papers in the comparison reference group. My best/central guess is that this paper falls into the 65th percentile for this.

I'll try to come up with the narrowest set of bounds that contain the range of values that has a 90% chance of containing what was called the "ideal correct rating". I see this 90% as being “very likely, but not certain”. So I'm thinking about my rating now, and what that rating might end up being if I had 100 years and vast resources to focus solely on getting this rating right, and if I had perfect knowledge, wisdom, judgment, etc.&#x20;

I've made intuitive judgments on questions like this in the past. I sometimes changed my mind a bit. Considering this in context, I am only somewhat confident in my judgment here. I'm unsure about the diagnostic tests for the two-way fixed effects. I'd put about a 10% probability that this work is actually in the bottom 45% of all work submitted to such journals. On the other hand, if these diagnostic tests were powerful, this would be among the strongest work in this respect. Thus, I'd give a 10% chance that this is in the top 10% of such work in this sense.

Thus, I give a central score of 65 for this metric, with 90% bounds (45, 90).

</details>

<details>

<summary>Am I setting these bounds well? (How is my calibration—the accuracy of my own uncertainty?)</summary>

"But how do I know if I'm setting these bounds right?"

One consideration is "calibration." If you're well-calibrated, then your specified 90% bounds should contain the true value close to 90% of the time. Similarly, 50% bounds should contain the true value half the time.\
\
If your 90% bounds contain the true value _less_ than 90% of of the time, you're being _overconfident_ (try to give wider bounds in the future). If they contain the true value more than 90% of the time, you are _underconfident_ (specify tighter bounds going forward).

To understand this better, assess your ability, then practice to get better at estimating your confidence in results. (See [calibrated probability assessment](https://www.wikiwand.com/en/Calibrated\_probability\_assessment) and tools like [calibrate your judgment](https://www.clearerthinking.org/tools/calibrate-your-judgment).)

"The aim of the web app is to help you become 'well-calibrated.' This means that when you say you’re 50% confident, you’re right about 50% of the time; when you say you're 90% confident, you're right about 90% of the time; and so on."

</details>

### Overall assessment

{% hint style="info" %}
We see "overall assessment" as the most important measure. Please prioritize this.
{% endhint %}

Judge the work’s quality heuristically. Consider all aspects of quality, importance to knowledge production, and importance to practice.&#x20;

### 0-100 Metric explained

_The description folded below focuses on the "Overall Assessment." Please try to use a similar scale when evaluating the category metrics._

{% hint style="info" %}
**Update Dec. 2023:** We are reframing these metrics to explicitly be considered in terms of percentiles relative to a reference group. _Further adjustment and explanation coming soon._&#x20;
{% endhint %}

<details>

<summary>Top ratings (90–100)</summary>

**95–100:** Among the highest quality and most important work you have ever read.

**90–100:** This work represents a major achievement, making substantial contributions to the field and practice. Such work would/should be weighed very heavily by tenure and promotion committees, and grantmakers.

_For example:_

* Most work in this area in the next ten years will be influenced by this paper.

<!---->

* This paper is substantially more rigorous or more insightful than existing work in this area in a way that matters for research and practice.

<!---->

* The work makes a major, perhaps decisive contribution to a case for (or against) a policy or philanthropic intervention.

</details>

<details>

<summary>Near-top (75–89) (*)</summary>

This work represents a strong and substantial achievement. It is highly rigorous, relevant, and well-communicated, up to the standards of the strongest work in this area (say, the standards of the top 5% of committed researchers in this field). Such work would/should not be decisive in a tenure/promotion/grant decision alone, but it should make a very solid contribution to such a case.

</details>

<details>

<summary>Middle ratings (40–59, 60–74) (*)</summary>

[**60–74.9**](#user-content-fn-6)[^6]**:** A very strong, solid, and relevant piece of work. It may have minor flaws or limitations, but overall it is very high-quality, meeting the standards of well-respected research professionals in this field.

**40–59.9:** A useful contribution, with major strengths, but also some important flaws or limitations.

</details>

<details>

<summary>Low ratings (5–19, 20–39) (*)</summary>

**20–39.9:** Some interesting and useful points and some reasonable approaches, but only marginally so. Important flaws and limitations. Would need substantial refocus or changes of direction and/or methods in order to be a useful part of the research and policy discussion.

**5–19.9:** Among the lowest quality papers; not making any substantial contribution and containing fatal flaws. The paper may fundamentally address an issue that is not defined or obviously not relevant, or the content may be substantially outside of the authors’ field of expertise.

**0–4:** Illegible, fraudulent, or plagiarized. _Please flag fraud, and notify us and the relevant authorities._

</details>

<details>

<summary>(*) 20 Mar 2023: We adjusted these ratings to avoid overlap</summary>

The previous categories were 0–5, 5–20, 20–40, 40–60, 60–75, 75–90, and 90–100. Some evaluators found the overlap in this definition confusing.

</details>

### The confidence/credible intervals: expressing uncertainty

#### **What are we looking for and why?**

We want policymakers and researchers to be able to _use The Unjournal'_s evaluations to carefully update their beliefs and make better decisions. To do this well, they need to weigh multiple evaluations against each other, and against other sources of information. How much weight should they give to each? In this context, it is important to _quantify the uncertainty_. That's why we ask you to provide a measure of this.&#x20;

_See box above_ ['But how do I actually come up with these ratings and confidence/credibility intervals?'](./#but-how-do-i-actually-come-up-with-these-ratings-and-confidence-credibility-intervals)



### **Category explanations: what you are rating**

[_Note that all of these criteria are scales (not binaries_](#user-content-fn-7)[^7]_)._

#### 1. Advancing our knowledge and practice

("To what extent"...) does the project make a contribution to the field or to practice, particularly in ways that will be relevant to our other criteria?

<details>

<summary>Less weight to "originality and cleverness’"</summary>

Originality and cleverness should be weighted less than the typical journal, because _The Unjournal_ focuses on **impact**. Papers that apply existing techniques and frameworks more rigorously than previous work or apply them to new areas in ways that provide **practical** insights for GP (global priorities) and interventions should be highly valued. More weight should be placed on contribution to GP than to the academic field.

</details>

Do the insights generated inform our ("posterior") beliefs about important parameters and about the effectiveness of interventions? [Note that we do not require a substantial _shift_ in our expectations; sound and well-presented "null results" can be valuable.](#user-content-fn-8)[^8]

Does the project leverage and incorporate recent relevant and credible work in useful ways?

#### 2. Methods: Justification, reasonableness, validity, robustness

Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are all of the given results justified in the discussion of methods?

Are the results and methods likely to be robust to reasonable changes in the underlying assumptions? [Does the author demonstrate this?](#user-content-fn-9)[^9]

Avoiding bias and questionable research practices (QRP): Did the authors take steps to reduce bias from opportunistic reporting and QRP? For example, pre-registration, multiple hypothesis testing corrections, and reporting flexible specifications.

<details>

<summary><em>What we mean by "methods"</em></summary>

We use the term “methods” here broadly: it may include choice/collection of data, experiment or survey design, statistical analysis, and simulation, among other elements.

</details>

#### 3. Logic and communication

_Coherent and clear argumentation, communication, reasoning transparency_

Are the goals/questions of the paper clearly expressed? Are concepts clearly defined and referenced?

Is the reasoning "transparent"? (See, e.g., [Open Philanthropy's guide](https://www.openphilanthropy.org/research/reasoning-transparency/) on reasoning transparency.) Are all of the assumptions and logical steps made clear? Does the logic of the arguments make sense? Is the argument written well enough to make it easy to follow?

Are the data and/or analysis presented relevant to the arguments made? Are the stated conclusions/results consistent with the evidence (or theoretical results/proofs) presented? Are the tables/graphs/diagrams easy enough to understand in the context of the narrative (e.g., no errors in labeling)?

#### 4. Open, collaborative, replicable science and methods

_**4a. Replicability, reproducibility, data integrity**_

<details>

<summary><strong>Explanation</strong></summary>

Would another researcher be able to perform the same analysis and get the same results? Are the method and its details explained sufficiently, in a way that would enable easy and credible replication? For example, a full description of analysis, code and software provided, and statistical tests fully explained. Is the source of the data clear?

Is the necessary data made as widely available as possible? As applicable? Ideally, the cleaned data should also be clearly labeled and explained/legible.

\
_Optional_: Are we likely to be able to construct the output from the shared code (and data)? _Note_ that evaluators are not required to run or evaluate the code; this is at your discretion. However, having a quick look at some of the elements could be helpful. Ideally, the author should give code that allows easy, full replicationl; for example, a single R script that runs and creates everything, starting from the original data source, and including data cleaning files. This would make it fairly easy for an evaluator to check. For example, see[ this taxonomy of "levels of computational reproducibility](https://bitss.github.io/ACRE/assessment.html#score)."

</details>

_**4b. Consistency**_

Do the numbers in the paper (and code output, if checked) make sense? Are they internally consistent throughout the paper?

_**4c. Useful building blocks**_

Do the authors provide tools, resources, data, and outputs that are likely to enable and enhance future work and meta-analysis?

#### 5. Engaging with real-world, impact quantification; practice, realism, and relevance

[Does the paper consider the real-world relevance of the arguments and results presented, perhaps engaging policy and implementation questions?](#user-content-fn-10)[^10]

Is the setup particularly well-informed by real-world norms and practices? “Is this realistic; does it make sense in the real world?”

<details>

<summary>Optional, desirable, invited</summary>

Authors might be encouraged—and should be rewarded—for the following:

* Do the authors communicate their work in ways policymakers and decision-makers are likely to understand (perhaps in a supplemental "non-technical abstract"), without being misleading and oversimplifying?

<!---->

* Do the authors present practical "impact quantifications," such as cost-effectiveness analyses, or provide results enabling these?

In future we may be able to pay them to do the above, if grant funding permits.

</details>

#### 6. Relevance to global priorities

Is this topic, approach, and discussion _potentially_ useful to global priorities research and interventions?

## Journal/prediction metrics

We would like to benchmark our evaluations against "how research is currently judged." We want to provide a bridge between the current _accept-or-reject_ system and an evaluation-based system. We want our evaluations to be taken seriously by universities and policymakers. Thus, we are asking you for _two_ "predictions" in the table below. The first is a "real-world" prediction, and the second is a comparable measure for a hypothetical "ideal world."

<table><thead><tr><th width="248">Journal/prediction metrics</th><th width="286" data-type="number">Predict: journal quality* (0.0–5.0)</th><th width="96">90% CI</th><th data-hidden data-type="select"></th><th data-hidden></th></tr></thead><tbody><tr><td>What "quality journal" do you expect this work <em>will</em> be published in?</td><td>null</td><td><em>lower, upper</em></td><td></td><td></td></tr><tr><td></td><td>null</td><td></td><td></td><td></td></tr><tr><td>Overall assessment on "scale" of journals, i.e., quality level of journal it <em>should</em> be published in.</td><td>null</td><td><em>lower, upper</em></td><td></td><td></td></tr><tr><td></td><td>null</td><td></td><td></td><td></td></tr><tr><td>*Note: <em>0=lowest/none; 5=highest/best. See</em> <a href="./#journal-metrics"><em>below</em></a> <em>for some benchmarks and guidelines.</em></td><td>null</td><td></td><td></td><td></td></tr></tbody></table>



{% hint style="info" %}
\*To better understand what we are asking here, please consult the subsections below:\
_Journal metrics;_ _In what quality level of journal . . . ;_ and _Overall assessment on "scale of journals"_
{% endhint %}

### Journal metrics (as a _continuous_ scale)

For the "prediction" questions above, we are asking for a journal quality rating prediction from 0.0 to 5.0. You can specify up to two digits (e.g., “4.4” or “2.0”). We are using this 0.0–5.0 metric here (rather than 0–100) as we suspect it is more familiar to academics.

{% hint style="info" %}
_**Remember**,_ we would like you to think of this as a _**continuous scale**_. E.g., if a paper/project would be most likely to be (or merits being) published in a journal that would rank about halfway between a top tier 'A journal' and a second tier (4/5) journal, you should rate it a 4.5.&#x20;

Please also use this continuous scale for your confidence/credible intervals.
{% endhint %}



**The metrics are:**

0/5: Marginally respectable/Little to no value; not publishable in any journal with scrutiny or credible WP series; not likely to be cited by credible researcher

1/5: OK/Somewhat valuable journal

2/5 Marginal B-journal/Decent field journal

3/5: Top B-journal/Strong field journal

4/5: Marginal A-Journal/Top field journal

5/5: A-journal/Top journal

We give some example journals [here](https://docs.google.com/spreadsheets/d/1nnS0FMOIKz-3rFJMn-JsX-6\_oE54gdvCjxs0vac6WF8/edit#gid=0) that may correspond to the above, based on SJR and ABS ratings.

### In what "quality level" of journal do you expect this work ultimately to be published?

<details>

<summary>What if this work has already been "peer-review published"?</summary>

The question above presumes that this work has not already been published in a peer-reviewed journal. However, we are planning to commission at least some post-publication review going forward. If the work has already been peer-review-published, you can either:

* Skip this question, _but please still answer the next prediction question_, the [#overall-assessment-on-scale-of-journals](./#overall-assessment-on-scale-of-journals "mention") or
* Answer a related question (not a prediction): “Suppose this paper were submitted to journals, in succession, from the top tier downwards. Imagine there is some randomness in this process. Consider all possible “random draws of the world.” In the "median draw," in what quality level journal would this paper be published?

</details>

<details>

<summary>"As if you were advising an author"</summary>

In presenting your prediction and confidence interval for same, you might want to consider if you were offering advice to an author:

* “What journal would be likely to publish this work?”
* “What is the most prestigious journal that would consider publishing this?”
* “What is the least prestigious journal that the authors should consider submitting this to?" (I.e., "I wouldn't go lower, even if I were risk-averse.”)

</details>

### **Overall **_**assessment**_** on "scale of journals"**

Consider the scale of journals described above. Suppose that

1. the journal process was fair, unbiased, and free of noise, and that status, social connections, and lobbying to get the paper published didn’t matter;
2. journals assessed research according to the category metrics we discussed above; and
3. this research was being submitted to journals according [to this fair process.\*](#user-content-fn-11)[^11]

_In such a case, in what quality level of journal would and should this research be published in its current form (or with minor revisions)?_

## Survey questions

{% hint style="info" %}
_For the questions below, we will publish your responses and review unless you ask us to keep them anonymous._
{% endhint %}

1. How long have you been in this field?
2. How many proposals and papers have you evaluated? _(For journals, grants, and other peer review.)_

_Your answers to the questions_ [_below will not be made public:_](#user-content-fn-12)[^12] &#x20;

1. How would you rate this template and process?
2. Do you have any suggestions or questions about this process or _The Unjournal_? (We will try to respond and to incorporate your suggestions.) \[Open response]
3. Would you be willing to consider evaluating a revised version of this project?

## **How to write a good review (general conventional guidelines)**

<details>

<summary>Some general key points to consider</summary>

* Cite evidence and reference specific parts of the research when giving feedback.
* Try to justify your critiques and claims in a reasoning-transparent way, rather than merely ‘"passing judgment."
* Provide specific, actionable feedback to the author where possible.
* When considering the authors’ arguments, consider the most reasonable interpretation of what they have written (and state what that is, to help the author make their point more clearly). See [steelmanning](https://www.lesswrong.com/tag/steelmanning).
* Be collegial and encouraging, but also rigorous. Criticize and question specific parts of the research without suggesting criticism of the _researchers themselves._

</details>

We are happy for you to use whichever process and structure you feel comfortable with when writing a peer review.

<details>

<summary>One possible structure</summary>

_Core_

* Assign an overall score based on quantitative metrics (possible: brief discussion of these metrics).
* Summarize the work and issues, and the research in context to convey your understanding and help others understand it.
* Highlight positive aspects of the paper, strengths and contributions.
  * Assess the contribution of the work in context of existing research.
* Note major limitations and potential ways the work could be improved; where possible, reference methodological literature and discussion and work that models what you are suggesting.

_Optional_

* Discuss minor flaws and their potential revisions.
  * You are not obliged (or paid) to spend a great deal of time copyediting the work. If you like, you can give a few specific suggestions and then suggest that the author look to make other changes along these lines.
* Offer suggestions for research agendas, increasing the impact of the work, incorporating the work into global priorities research and impact evaluations, and enhancing future work.

</details>

{% hint style="info" %}
**Remember**: _The Unjournal_ doesn’t “publish” and doesn’t “accept or reject.” So don’t give an _Accept_, _Revise and Resubmit_, or _Reject_-type recommendation. We just want quantitative metrics, some written feedback, and some relevant discussion.
{% endhint %}

<details>

<summary>"This paper is great, I would accept it without changes, what should I write/do?"</summary>

_We still want your evaluation and ratings. Some things to consider as an evaluator in this situation:_

1. We still want your quantitative ratings and predictions.
2. A paper or project is not only a good to be judged on a single scale. How useful is it, and to whom or what? We'd like you discuss its value in relation to previous work, it’s implications, what it suggests for research and practice, etc.
3. Even if the paper is great . . .
   * Would you accept it in the “top journal" in economics”? If not, why not?
   * Would you hire someone based on this paper?
   * Would you fund a major intervention (as a government policymaker, major philanthropist, etc.) based on this paper alone? If not, why not?
4. What are the most important and informative results of the paper?
5. Can you quantify your confidence in these "crucial" results, and their replicability and generalizability to other settings? Can you state your probabilistic bounds (confidence or credibility intervals) on the quantitative results (e.g., 80% bounds on QALYs/DALYs/or WELLBYs per $1,000).
6. Would any other robustness checks or further work have the potential to increase your confidence (narrow your belief bounds) in this result? Which?
7. Do the authors make it easy to reproduce the statistical (or other) results of the paper from shared data? Could they do more in this respect?
8. Communication: Did you understand all of the paper? Was it easy to read? Are there any parts that could have been better explained?
   * Is it communicated in a way that would it be useful to policymakers? To other researchers in this field, or in the general discipline?

</details>

## Writing referee reports: resources and benchmarks

**Economics**\
[Econometric Society: Guidelines for referees](https://www.econometricsociety.org/publications/econometrica/browse/guidelines-referees)

[How to Write an Effective Referee Report and Improve the Scientific Review Process (Berk et al, 2017)](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.31.1.231)

Report: [Improving Peer Review in Economics: Stocktaking and Proposal (Charness et al 2022)](https://evalresearch.weebly.com/uploads/1/3/3/4/133478410/improving\_peer\_review\_in\_economics\_-\_charness\_et\_al..pdf)

**Open Science**

[PLOS](https://plos.org/resource/how-to-write-a-peer-review/) (Conventional but open access; simple and brief)

[Peer Community In... Questionnaire ](https://peercommunityin.org/2022/05/20/questionnaire-for-reviewers/)(Open-science-aligned; perhaps less detail-oriented than we are aiming for)

[Open Reviewers Reviewer Guide ](https://zenodo.org/record/5484087)(Journal-independent “pre-review”; detailed; targets ECRs)

**General**

[The Wiley Online Library](https://authorservices.wiley.com/Reviewers/journal-reviewers/how-to-perform-a-peer-review/step-by-step-guide-to-reviewing-a-manuscript.html) (Conventional; general)

[^1]: Updated 14 July 2023

[^2]: We will agree on a scheduled deadline. Generally, we aim for a three-week turnaround. Evaluations submitted after the agreed deadline (but still in a reasonable window) will earn a $300 honorarium.

[^3]: We are using a Google Doc for now; we're hoping to move to asking the PubPub  interface soon. But we will still be flexible to let you link or upload your descriptive, free-form evaluation in whatever form you prefer.           &#x20;

[^4]: These are described below in general; e.g., we value 'novelty' less than the credibility of the methods and reported results. We also may give specific suggestions and guidelines for specific research.

[^5]: You may want to glance at these tables before writing your report, to gain a sense of our priorities.

[^6]: This previously read "60-75"; we adjusted this because some evaluators found the overlap unclear.

[^7]: _So, consider each of the questions below to be prefaced with "to what extent"._

[^8]: If research is conceptually and methodologically sound (including being adequately powered), carefully presented ‘null results’ can, in general, advance the field. I.e., (in Bayesian terminology) work that “substantially concentrates our belief distributions around the initial expectations” is also highly valuable.

[^9]: E.g., did they give at least a reasonable range of robustness checks? At best, did they ‘map the space’ of possible reasonable specifications?

[^10]: E.g., does it help us evaluate what to prioritize for interventions and policy, improve interventions and policy, or improve our research and knowledge capacity for these?

[^11]: \*This third element was added in June 2023.

[^12]: 18 Sep 2023: we intend to summarize the comments in aggregate, but not in a way that you are identifiable.    &#x20;

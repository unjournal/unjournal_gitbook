---
description: >-
  This page describes The Unjournal's evaluation guidelines, considering our
  priorities and criteria, the metrics we ask for, and how these are considered.
---

# Guidelines for evaluators

{% hint style="info" %}
To download a printable version of these guidelines,  click the three dots (**⋮**) above and choose **Export as PDF**.
{% endhint %}

{% hint style="info" %}
Please see  [for-prospective-evaluators.md](../for-prospective-evaluators.md "mention") for an overview of this process, as well as details on compensation, public recognition, and more
{% endhint %}



## What we would like you to do

1. **Write an evaluation of the target paper,** similar to a standard, high-quality referee report.
2. [**Give quantitative metrics and predictions** as described below.](#user-content-fn-1)[^1]
3. Answer a short questionnaire about your background and our processes.

## Writing the evaluation (aka 'the review')

In writing your evaluation and providing ratings, please consider the following:

### _The Unjournal_'s criteria

Broadly, the review should be similar to a report an academic would write for a traditional high-prestige journal (e.g., see some 'conventional guidelines' [here](conventional-guidelines-for-referee-reports.md)). Specifically, we'd like you to focus on our priorities:

1. Advancing our knowledge and practice
2. Justification, reasonableness, validity, and robustness of methods
3. Logic and communication
4. Open, communicative, replicable science

See our [guidelines below](./#overall-assessment) for more details on each of these. We are not asking you to _structure_ your review according to these metrics, just to pay extra attention to them.

<details>

<summary>Specific requests for focus or feedback</summary>

Please pay attention to anything our managers and editors specifically asked you to focus on. We may ask you to focus on specific areas of expertise. We may also forward specific feedback requests from authors.

</details>

<details>

<summary>The evaluation will be made public</summary>

Unless you were advised otherwise, this evaluation, including the review and quantitative metrics, will be given a DOI and, hopefully, will enter the public research conversation. Authors will be given two weeks to respond to reviews before the evaluations, ratings, and responses are made public. You can choose whether you want to be identified publicly as an author of the evaluation.

</details>

If you have questions about the authors’ work, you can ask them anonymously; we will facilitate this.&#x20;

We want you to evaluate the _most recent/relevant version of the paper/project that you can access_. If you see a more recent version than the one we shared with you, please let us know.&#x20;

<details>

<summary>Publishing and signing reviews: considerations and exceptions</summary>

We may give early-career researchers the right to veto the publication of very negative reviews or to embargo the release of these reviews for a defined period. We will inform you in advance if this will be the case for your evaluation.

You can reserve some "sensitive" content in your report to be shared with only _The Unjournal_ management or only the authors, but we hope to keep this limited.

</details>

### **The target audiences for these evaluations; what to prioritize**

We designed this process to balance three considerations; we ask evaluators to consider each of these, and three 'target audiences':&#x20;

1. Crafting evaluations and ratings that help researchers and policymakers judge when and how to rely on this research. For _Research Users._
2. Ensuring these evaluations of the _papers_ are comparable to current journal tier metrics, to enable them to be used to determine career advancement and research funding. For _Departments, Research_ _Managers, and Funders._&#x20;
3. Providing constructive feedback to _Authors_.

We discuss this, and how it relates to our impact and "theory of change", [here](../../../faq-interaction/referees-evaluators.md#the-value-of-these-evaluations-what-to-prioritize).



## Quantitative metrics

We ask for a set of nine quantitative metrics (see table below). For each metric, we ask for a rating and a 90% credible interval. We describe these in detail below.

<table><thead><tr><th width="374">Quantitative metric</th><th width="142" align="center">Scale</th><th align="center">Metric</th><th align="center">Type</th><th data-hidden></th></tr></thead><tbody><tr><td>Overall assessment</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating (heuristic)</td><td></td></tr><tr><td>Advancing our knowledge and practice</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating</td><td></td></tr><tr><td>Methods: Justification, reasonableness, validity, robustness</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating</td><td></td></tr><tr><td>Logic and communication</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating</td><td></td></tr><tr><td>Open, collaborative, replicable science and methods</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating</td><td></td></tr><tr><td>Engaging with real-world, impact quantification; practice, realism, relevance</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating</td><td></td></tr><tr><td>Relevance to global priorities</td><td align="center">0.0% - 100%</td><td align="center">Percentile</td><td align="center">Rating</td><td></td></tr><tr><td>Overall assessment on the journal-tier scale, i.e., what journal-quality tier should this work be published in?</td><td align="center">0.0 - 5.0 </td><td align="center">Defined tiers</td><td align="center">Rating (heuristic)</td><td></td></tr><tr><td>What journal-quality tier do you expect this work <em>will</em> be published in?</td><td align="center">0.0 - 5.0 </td><td align="center">Defined tiers</td><td align="center">Prediction</td><td></td></tr></tbody></table>



For some questions, we ask for a **percentile ranking** from 0-100%. This represents "what proportion of papers in the reference group are worse than this paper, by this criterion". A score of 100% means this is essentially the best paper in the reference group. 0% is the worst paper. A score of 50% means this is the median paper; i.e., half of all papers in the reference group do this better, and half do this worse, and so on.

Here, the population of papers should be _**all serious research in the same area that you have encountered in the last three years.**_

<details>

<summary>"Serious" research? Academic research? </summary>

Here, we are mainly considering research done by professional researchers with high levels of training, experience, and familiarity with recent practice, who have time and resources to devote months or years to each such research project or paper. These will typically be written as 'working papers' and presented at academic seminars, before being submitted to standard academic journals. Although no credential is required, this typically includes people with PhD degrees (or upper-level PhD students). Most of this sort of research is done by full-time academics (professors, post-docs, academic staff, etc.) with a substantial research remit, as well as research staff at think tanks and research institutions (but there may be important exceptions). &#x20;

</details>

#### What counts as the "same area"?

This is a judgment call. Here are some criteria to consider: first and foremost, does the work come from the same academic field and research subfield, and does it address questions that might be addressed using similar methods? Secondly, does it deal with the same substantive research question, or perhaps a closely related one? If the research you are evaluating is in a very niche topic, the comparison reference group should be expanded to consider work in other areas.&#x20;

<details>

<summary>"Research that you have encountered"</summary>

We are aiming for comparability across evaluators. If you  suspect that you are particularly exposed to higher-quality work in this category, compared to other likely evaluators, you may want to adjust your reference group downwards. (And of course vice-versa, if you suspect you are particularly exposed to lower-quality work.)

</details>

#### Midpoint rating and credible intervals&#x20;

For each rating (or prediction) we ask you to provide a [_credible interval_](https://www.wikiwand.com/en/Credible\_interval) for this rating, as a measure of your confidence or uncertainty. Our PubPub interface provides "slider bars" to express your chosen intervals, as in the image below. This form will ask you to "please indicate your rating (the middle slider or dot), and the lower and upper endpoints of your 90% credible interval (the lower and upper dots).

<figure><img src="../../../.gitbook/assets/image (16).png" alt=""><figcaption><p>A "Slider bar" for a rating, as implemented in PubPub V7</p></figcaption></figure>

{% hint style="info" %}
_We recognize that you may not be used to quantifying your uncertainty in this way._ [_Please see below_](./#the-credible-intervals-expressing-uncertainty) _for further explanation and guidance._
{% endhint %}

###

### Overall assessment

_Percentile ranking (0-100%)_

Judge the quality of the research heuristically. Consider all aspects of quality, credibility, importance to knowledge production, and importance to practice.&#x20;

### Advancing our knowledge and practice

_Percentile ranking (0-100%)_

To what extent does the project contribute to the field or to practice, particularly in ways that will be relevant to our other criteria?

<details>

<summary>Less weight to "originality and cleverness’"</summary>

Originality and cleverness should be weighted less than the typical journal, because _The Unjournal_ focuses on _impact_. Papers that apply existing techniques and frameworks more rigorously than previous work or apply them to new areas in ways that provide practical insights for GP (global priorities) and interventions should be highly valued. More weight should be placed on 'contribution to GP' than on 'contribution to the academic field'.

</details>

Do the insights generated inform our beliefs about important parameters and about the effectiveness of interventions? **W**[**e do not require surprising results; sound and well-presented "null results" can be valuable.**](#user-content-fn-2)[^2]

Does the project add useful value to other relevant and credible impactful research?

### Methods: Justification, reasonableness, validity, robustness

_Percentile ranking (0-100%)_

Are the methods used well-justified and explained; are they a reasonable approach to answering the question(s) in this context? Are the underlying assumptions reasonable? Are all of the given results justified in the discussion of methods?

Are the results and methods likely to be robust to reasonable changes in the underlying assumptions? [Does the author demonstrate this?](#user-content-fn-3)[^3]

Avoiding bias and questionable research practices (QRP): Did the authors take steps to reduce bias from opportunistic reporting and QRP? For example, did they do a strong pre-registration and pre-analysis plan, incorporate multiple hypothesis testing corrections, and report flexible specifications?

<details>

<summary>What we mean by "methods"</summary>

We use the term “methods” here broadly: it may include choice/collection of data, experiment or survey design, statistical analysis, and simulation, among other elements.

</details>

### Logic and communication

_Percentile ranking (0-100%)_

Are the goals/questions of the paper clearly expressed? Are concepts clearly defined and referenced?

Is the reasoning "transparent"? (See, e.g., [Open Philanthropy's guide](https://www.openphilanthropy.org/research/reasoning-transparency/) on reasoning transparency.) Are all of the assumptions and logical steps made clear? Does the logic of the arguments make sense? Is the argument written well enough to make it easy to follow?

Are the stated conclusions/results consistent with the evidence (or theoretical results/proofs) presented?&#x20;

Are the data and/or analysis presented relevant to the arguments made? Are the tables, graphs, and diagrams easy to understand in the context of the narrative (e.g., no major errors in labeling)?

### Open, collaborative, replicable science and methods

_Percentile ranking (0-100%). This covers several considerations, outlined below._

#### _**Replicability, reproducibility, data integrity**_

Would another researcher be able to perform the same analysis and get the same results? Are the method and its details explained sufficiently, in a way that would enable easy and credible replication? For example, providing a full description of analysis, code and software, with statistical tests fully explained.&#x20;

Is the source of the data clear?

Is the necessary data made as widely available as possible? As applicable? Ideally, the cleaned data should also be clearly labeled and explained/legible.

<details>

<summary><em>Optional</em>: Are we likely to be able to construct the output from the shared code (and data)? </summary>

_Note_ that evaluators are not required to run or evaluate the code; this is at your discretion. However, having a quick look at some of the elements could be helpful. Ideally, the author should give code that allows easy, full replication; for example, a single R script that runs and creates everything, starting from the original data source, and including data cleaning files (even better if '[containerized/dockerized](https://aeadataeditor.github.io/posts/2021-11-16-docker)' to be platform-independent). This would make it fairly easy for an evaluator to check. For example, see[ this taxonomy of "levels of computational reproducibility](https://bitss.github.io/ACRE/assessment.html#score)."

</details>

&#x20;_**Consistency**_

Do the numbers in the paper (and the code output, if checked) make sense? Are they internally consistent throughout the paper?

_**Useful building blocks**_

Do the authors provide tools, resources, data, and outputs that are likely to enable and enhance future work and meta-analysis?

### Engaging with real-world, impact quantification; practice, realism, relevance

_Percentile ranking (0-100%)_

[Does the paper consider the real-world relevance of the arguments and results presented, perhaps engaging policy and implementation questions?](#user-content-fn-4)[^4]

Is the setup particularly well-informed by real-world norms and practices? “Is this realistic; does it make sense in the real world?”

<details>

<summary>Optional, desirable, invited</summary>

Authors might be encouraged—and should be rewarded—for the following:

* Do the authors communicate their work in ways policymakers and decision-makers are likely to understand (perhaps in a supplemental "non-technical abstract"), without being misleading and oversimplifying?

<!---->

* Do the authors present practical "impact quantifications," such as cost-effectiveness analyses, or provide results enabling these?

In future we may be able to pay them to do the above, if grant funding permits.

</details>

### Relevance to global priorities

_Percentile ranking (0-100%)_

Is the topic and paper _potentially_ useful [to informing global priorities and high-impact interventions?](../../../the-field-and-ea-gp-research.md)



## Journal-tier metrics and predictions

We would like to benchmark our evaluations against 'how research is currently judged.' We want to provide a bridge between the current _accept-or-reject_ system and an evaluation-based system. We want our evaluations to be taken seriously by universities and policymakers. Thus, we are asking you for _two_ assessments in the table below, on a journal-tier scale. The first is a normative judgment question. The second is an actual prediction for the real-world publication outcome.&#x20;

<table><thead><tr><th width="364">Journal-tier metrics</th><th width="248" align="center">Journal-tier scale* (0.0–5.0)</th><th width="238">90% CI</th><th data-hidden><select></select></th><th data-hidden></th></tr></thead><tbody><tr><td><a href="./#overall-assessment-on-the-journal-tier-scale">Overall assessment on the journal-tier scale</a>, i.e., what journal-quality tier <em>should</em> this work be published in?</td><td align="center">X.xx </td><td><em>lower, upper</em></td><td></td><td></td></tr><tr><td>What journal-quality tier do you expect this work <em>will</em> be published in?</td><td align="center">X.xx</td><td><em>lower, upper</em></td><td></td><td></td></tr><tr><td></td><td align="center"></td><td></td><td></td><td></td></tr><tr><td>*Note: <em>0=lowest/none; 5=highest/best.</em> </td><td align="center"><em>See</em> <em>below</em> <em>for some benchmarks and guidelines.</em></td><td></td><td></td><td></td></tr></tbody></table>

For the "Journal-tier" questions above, we are asking for a rating or prediction from 0.0 to 5.0 (see further discussion and benchmarks below). You can specify up to two digits (e.g., “4.4” or “2.0”). We use this 0.0–5.0 metric here (rather than 0–100) as we suspect it is more familiar to academics.&#x20;

{% hint style="info" %}
_**Remember**,_ we would like you to think of this as a _**continuous scale**_. E.g., if a paper/project would be most likely to be (or merits being) published in a journal that would rank about halfway between a top tier 'A journal' and a second tier (4/5) journal, you should rate it a 4.5. Please also use this continuous scale for providing _credible intervals_.
{% endhint %}

{% hint style="info" %}
**PubPub note:** as of 8 Feb 2024, the PubPub form is _not_ allowing you to give non-integer responses. Until this is fixed, please provide these (potentially) non-interval 'continuous' journal-metric predictions/ratings and CIs for these in the comment boxes below the slider.
{% endhint %}

#### **The metrics are:**

* 0/5: Marginally respectable/Little to no value; not publishable in any journal with scrutiny or credible WP series; not likely to be cited by credible researcher
* 1/5: OK/Somewhat valuable journal
* 2/5 Marginal B-journal/Decent field journal
* 3/5: Top B-journal/Strong field journal
* 4/5: Marginal A-Journal/Top field journal
* 5/5: A-journal/Top journal

We give some example journals [here](https://docs.google.com/spreadsheets/d/1nnS0FMOIKz-3rFJMn-JsX-6\_oE54gdvCjxs0vac6WF8/edit#gid=0) that may correspond to the above, based on SJR and ABS ratings.

### **Overall **_**assessment**_** on the journal-tier scale; what journal-quality tier should this work be published in?**

Assess this paper on the journal-tier scale described above, considering only its merit, particularly considering the category metrics we discussed above.

Equivalently, suppose that

1. the journal process was fair, unbiased, and free of noise, and that status, social connections, and lobbying to get the paper published didn’t matter;
2. journals assessed research according to the category metrics we discussed above; and
3. this research was being submitted to journals according to this fair process.

_In such a case, at what quality-level tier journal would and should this research be published in its current form (or with minor revisions)?_

#### Give a number between 0.0 and 5.0, along with a 90% CI.

### What journal-quality tier journal do you expect this work _will_ be published in?

<details>

<summary>What if this work has already been "peer-review published"?</summary>

The question above presumes that this work has not already been published in a peer-reviewed journal. However, we are planning to commission at least some post-publication review going forward. If the work has already been peer-review-published, you can either:

* Skip this question, _but please still answer the next prediction question_, the [#overall-assessment-on-scale-of-journals](./#overall-assessment-on-scale-of-journals "mention") or
* Answer a related question (not a prediction): “Suppose this paper were submitted to journals, in succession, from the top tier downwards. Imagine there is some randomness in this process. Consider all possible “random draws of the world.” In the "median draw," in what quality level journal would this paper be published?

</details>

<details>

<summary>"As if you were advising an author"</summary>

In presenting your prediction and confidence interval for same, you might want to consider if you were offering advice to an author:

* “What journal would be likely to publish this work?”
* “What is the most prestigious journal that would consider publishing this?”
* “What is the least prestigious journal that the authors should consider submitting this to?" (I.e., "I wouldn't go lower, even if I were risk-averse.”)

</details>

#### Give a number between 0.0 and 5.0, along with a 90% CI



## The credible intervals: expressing uncertainty

#### **What are we looking for and why?**

We want policymakers and researchers to be able to _use The Unjournal'_s evaluations to carefully update their beliefs and make better decisions. To do this well, they need to weigh multiple evaluations against each other, and against other sources of information. Evaluators may feel very confident in their rating on a particular category, but far less confident in another area. How much weight should they give to each? In this context, it is important to _quantify the uncertainty_. That's why we ask you to provide a measure of this.&#x20;

<details>

<summary>Why are you asking about "confidence" in these metrics?</summary>

We would like you to state your "confidence intervals" or "credibility intervals." Loosely speaking, we hope to capture a sense of how sure you are about your ratings. This will help people who read your evaluation to know how much weight to put on them in using them for making their own decisions. These can also be used in systematic ways for meta-science and meta-analysis. We can "aggregate expert judgment" to get a better measure of how confident we _should be_ about particular measures and claims.

</details>

<details>

<summary>But how do I <em>actually</em> come up with these confidence/credibility intervals (and the ratings themselves)?</summary>

You may know most of the concepts below, but you might be unfamiliar with applying them in a situation like this one.

Suppose your best guess for the "Methods..." criterion is 65. Still, even an expert can never be certain. E.g., you may misunderstand some aspect of the paper, there may be a justification or method you are not familiar with, you might not understand the criterion, etc.

Your "uncertainty" over this could be described by some distribution, representing your beliefs about the _true value_ of this criterion. By "true value," you might think, "If you had all the evidence, knowledge, and wisdom in the world, and the benefit of perfect hindsight, what value would you choose for this criterion?"

Your "'best guess" should (basically) be the central mass point of this distribution.

You are asked to give a 90% interval. Loosely speaking, you could consider something like, "What is the smallest interval around this best guess that I believe is 90% likely to contain the true value?"

_**E.g., you might have thoughts similar to these:**_

"I am going to interpret the 'methods' in terms of their reliability for consistent causal inference and minimizing parameter mean-squared error in settings like this one."

"I will consider the appropriateness of the methods chosen relative to the choices made across the distribution of all papers in the comparison reference group. My best/central guess is that this paper falls into the 65th percentile for this."

"I'll try to come up with the narrowest set of bounds that contain the range of values that has a 90% chance of containing what was called the "ideal correct rating". I see this 90% as being “very likely, but not certain”. So I'm thinking about my rating now, and what that rating might end up being if I had 100 years and vast resources to focus solely on getting this rating right, and if I had perfect knowledge, wisdom, judgment, etc."&#x20;

"I've made intuitive judgments on questions like this in the past. I sometimes changed my mind a bit. Considering this in context, I am only somewhat confident in my judgment here. I'm unsure about the diagnostic tests for the two-way fixed effects. I'd put about a 10% probability that this work is actually in the bottom 45% of all work submitted to such journals. On the other hand, if these diagnostic tests were powerful, this would be among the strongest work in this respect. Thus, I'd give a 10% chance that this is in the top 10% of such work in this sense."

"Thus, I give a central score of 65 for this metric, with 90% bounds (45, 90)."

</details>

<details>

<summary>Am I setting these bounds well? (How is my calibration—the accuracy of my own uncertainty?)</summary>

_"But how do I know if I'm setting these bounds right?"_

One consideration is "calibration." If you're well-calibrated, then your specified 90% bounds should contain the true value close to 90% of the time. Similarly, 50% bounds should contain the true value half the time.\
\
If your 90% bounds contain the true value _less_ than 90% of of the time, you're being _overconfident_ (try to give wider bounds in the future). If they contain the true value more than 90% of the time, you are _underconfident_ (specify tighter bounds going forward).

To understand this better, assess your ability, then practice to get better at estimating your confidence in results. (See [calibrated probability assessment](https://www.wikiwand.com/en/Calibrated\_probability\_assessment) and tools like [calibrate your judgment](https://www.clearerthinking.org/tools/calibrate-your-judgment).)

_"The aim of the web app is to help you become 'well-calibrated.' This means that when you say you’re 50% confident, you’re right about 50% of the time; when you say you're 90% confident, you're right about 90% of the time; and so on."_

This web app may help you understand the idea of calibration and improve your calibration in a general sense.&#x20;

We aim to build further tools and exercises that will be specific to _The Unjournal's_ evaluation context; to help you understand our metrics, and improve your accuracy as well as your   calibration.&#x20;

</details>



## Survey questions

At the bottom of the form, we ask evaluators a few questions about their background, and ask them for feedback.

<details>

<summary>Survey questions for evaluators: details</summary>

For _the_ two questions below, we will publish your responses and review unless you ask us to keep them anonymous.

1. How long have you been in this field?
2. How many proposals and papers have you evaluated? _(For journals, grants, and other peer review.)_



_Answers to the questions_ [_below will not be made public:_](#user-content-fn-5)[^5] &#x20;

1. How would you rate this template and process?
2. Do you have any suggestions or questions about this process or _The Unjournal_? (We will try to respond to your suggestions, and incorporate them in our practice.) \[Open response]
3. Would you be willing to consider evaluating a revised version of this project?

</details>



## Other guidelines and notes

<details>

<summary>Note on the evaluation platform (13 Feb 2024)</summary>

12 Feb 2024: We are moving to a hosted form/interface in PubPub. That form is still somewhat a work-in-progress, and may need some further guidance; we try to provide this below, but please contact us with any questions. [If you prefer](#user-content-fn-6)[^6], you can also submit your response in a Google Doc**,** and share it back with us. Click [here](https://docs.google.com/document/d/1erOQ8qiWmgAmd9WdMLmuBGoxFkUJeQo2c8pc5wFAQbk/copy) to make a new copy of that  directly.  &#x20;

</details>



**Length/time spent:** This is up to you. We welcome detail, elaboration, and technical discussion.

<details>

<summary>Length and time: possible benchmarks</summary>

[The Econometrics society](https://www.econometricsociety.org/publications/econometrica/browse/guidelines-referees) recommends a 2–3 page referee report; [Berk et al.](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.31.1.231) suggest this is relatively short, but confirm that brevity is desirable. [In a recent survey (Charness et al., 2022)](https://evalresearch.weebly.com/report---full-text.html), economists report spending (median and mean) about one day per report, with substantial shares reporting "half a day" and "two days." We expect that reviewers tend spend more time on papers for high-status journals, and when reviewing work that is closely tied to their own agenda.

</details>

{% hint style="info" %}
**Adjustments:** We have made some adjustments to this page and to our guidelines and processes; this is particularly relevant for considering earlier evaluations. See [#adjustments-to-metrics-and-guidelines-previous-presentations](why-these-guidelines.md#adjustments-to-metrics-and-guidelines-previous-presentations "mention").&#x20;
{% endhint %}

<details>

<summary>Reference groups: future calibration work</summary>

The reference groups we state above, for considering the ratings, are  admittedly ad-hoc. We are working to develop benchmarks, vignettes and standards for these reference groups, to enable comparability while preserving the transparency of our quantitative 'percentiles' approach.&#x20;

</details>

<details>

<summary> Considering attributes and descriptions of research by percentile (a WIP)</summary>

We previously described the ratings scales qualitatively. E.g., "Top ratings (90–100)" were stated to represent "a major achievement, making substantial contributions to the field and practice. ... should be weighed very heavily by tenure and promotion committees, and grantmakers."&#x20;

We [link these descriptions here](why-these-guidelines.md#previous-descriptions-of-ratings-intervals), and you are free to consult them. Consider them "as our rough impressions of how these percentiles/ratings would break down".  However,  these attributes [might not actually describe the qualities of research in the relevant percentiles](#user-content-fn-7)[^7]; so we are de-emphasizing them.&#x20;

We plan to pursue this further, to work with evaluators, raters, and researchers, to develop and present empirically-grounded descriptions (from surveys and elicitations) of the _attributes_ of research that falls into the different deciles, relative to an agreed-upon reference group. &#x20;

</details>

_Our data protection statement is linked_ [_here_](https://bit.ly/46y0LqH)_._

[^1]: You may want to glance at these tables before writing your report, to gain a sense of our priorities.

[^2]: If research is conceptually and methodologically sound (including being adequately powered), carefully presented ‘null results’ can, in general, advance the field. I.e., (in Bayesian terminology) work that “substantially concentrates our posterior belief distributions around the initial expectations” is also highly valuable.

[^3]: E.g., did they give at least a reasonable range of robustness checks? At best, did they ‘map the space’ of possible reasonable specifications in a clear 'multiverse analysis'?

[^4]: E.g., does it help us evaluate what to prioritize for interventions and policy, improve interventions and policy, or improve our research and knowledge capacity for these?

[^5]: 18 Sep 2023: we intend to summarize the comments in aggregate, but not in a way that you are identifiable.    &#x20;

[^6]: We are replacing the Google Docs with the tailored PubPub interface; the PubPub form should be more efficient and leave less room for misunderstanding. But we will still be flexible to let you link or upload your descriptive, free-form evaluation in whatever form you prefer.

[^7]: We plan to pursue this further, to work with evaluators, raters, and researchers  be able to present empirically-grounded descriptions of the attributes of research that falls into the different deciles.&#x20;

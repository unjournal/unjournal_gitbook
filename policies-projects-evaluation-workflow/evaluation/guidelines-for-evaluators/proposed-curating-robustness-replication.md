# Proposed curating robustness replication

We are considering asking evaluators, with compensation, to assist and engage in the process of "robustness replication." This may lead to some interesting follow-on possibilities as we build our potential collaboration with the[ Institute for Replication](https://i4replication.org/) and others in this space.

We might ask evaluators discussion questions like these:

* _What is the most important, interesting, or relevant substantive claim made by the authors, (particularly considering global priorities and potential interventions and responses)?_
* _What statistical test or evidence does this claim depend on, according to the authors?_
* _How confident are_ you _in the substantive claim made?_
* "Robustness checks": _What specific statistical test(s) or piece(s) of evidence would make_ you _substantially more confident in the substantive claim made?_
* _If a robustness replication "passed" these checks, how confident would you be_ then _in the substantive claim? (You can also express this as a continuous function of some statistic rather than as a binary; please explain your approach.)_

**Background**:

The Institute for Replication is planning to hire experts to do "robustness-replications" of work published in a top journal in economics and political science. Code- and data sharing is now being enforced in many or all of these journals and other important outlets. We want to support their efforts and are exploring collaboration possibilities. We are also considering how to best guide potential future robustness replication work.

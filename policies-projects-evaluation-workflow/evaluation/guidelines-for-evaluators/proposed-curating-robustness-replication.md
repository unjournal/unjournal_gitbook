# Proposed curating robustness replication

We are considering asking (and compensating them for this) to assist and engage in the process of 'robustness replication'. This may lead to some interesting follow-on possibilities, as we build our potential collaboration with the[ Institute for Replication](https://i4replication.org/) and others in this space.

We might ask evaluators to identify and discuss:

* ‘What is the most important, interesting, or relevant substantive claim made by the authors, (particularly considering global priorities and potential interventions and responses)?’
* What statistical test or evidence does this claim depend on, according to the authors?
* How confident are _you_ in the substantive claim made?
* ‘Robustness checks’: What specific statistical test(s) or piece(s) of evidence would make _you_ substantially more confident in the substantive claim made?&#x20;
* If a robustness replication 'passed these checks', how confident would you _then_ be in the substantive claim? (You could also express this as a continuous function of some statistic rather than as a binary; please explain your approach).\
  \


_**Background**_:

The Institute for Replication is planning to hire experts to do ‘robustness-replications’ of work published in a top journal in economics and political science. Code and data sharing is now being enforced in many/all of these journals and other important outlets. We want to support their efforts, and we are exploring collaboration possibilities. We are considering how to best support them and guide potential future robustness replication work

&#x20;

---
description: (for pilot and beyond)
---

# What research to target?

Our [initial focus](#user-content-fn-1)[^1] is quantitative work that informs [global priorities (see linked discussion)](../../the-field-and-ea-gp-research.md), especially in [economics, policy, and social science](#user-content-fn-2)[^2]. We want to see better research leading to better concrete outcomes that enable or accelerate positive change.

_See (earlier) discussion in public call/EA forum discussion:_

{% embed url="https://forum.effectivealtruism.org/posts/kftzYdmZf4nj2ExN7/the-most-pivotal-empirical-pieces-of-research-you-would-like-2" %}

To reach these goals, we need to select "the right research" for evaluation. We want to choose papers and projects that are highly relevant, methodologically promising, and that will benefit substantially from our evaluation work. Besides making our process transparent and fair, we need to optimize how we select research so that our efforts remain mission-focused and _useful_. To do this, we need a coherent set of guidelines \[and ways of thinking] to guide this process. These considerations have several dimensions, which we explore below.

{% hint style="info" %}
_Management access only_: General discussion of prioritization in Gdoc [HERE](https://docs.google.com/document/d/1Ei-3t2bUazjlBOIg8chLxOrz\_sXl\_osAzrLU4hO1wDk/edit). Private discussion of specific papers in Airtable and links (e.g., [HERE](https://docs.google.com/document/d/14HXHQTqwJ5VOw-SBoJD8Sd3jathdO9geKdmhdOOx\_Gw/edit)).

_We incorporate some of this discussion below..._
{% endhint %}

## High-level considerations for considering research

When considering a piece of research to decide whether to commission it to be evaluated, we can start by looking at its _general relevance_ as well as the _value_ _of evaluating_ and rating it.

1. **Why is it relevant and worth engaging with?**

_Our prioritization of a paper for evaluation should not be seen as an assessment of its quality, nor of its 'vulnerability'_.  We consider (and prioritize) the importance of the research to global priorities; its relevance to crucial decisions;  the attention it is getting, the influence it is having; its direct relevance to the real world; and the potential value of the research for advancing other impactful work.  We de-prioritize work that has already been credibly (publicly) evaluated[^3].  We also consider the research's fit with our scope (social science, etc.), and the likelihood that we can commission experts to meaningfully evaluate it.

[_'The prioritization is not the evaluation', it is less_ ](#user-content-fn-4)[^4]_specific and less intensive._

_Some features we value, that might raise the probability we consider a paper or project:_  the commitment and contribution to open science.  the authors' engagement with our process; and the logic, communication, and transparent reasoning of the work. _However,_ if a prominent research paper within our scope seems to have a strong potential for impact, we will prioritize it highly, whether or not it has these features.

\
2\. **Why does it need (more) evaluation, and what are some key issues and claims to vet?**

_We ask the people who suggest particular research, and experts in the field:_

* What are (some of) the authors’ main important claims that are worth carefully evaluating?
* What aspects of the evidence, argumentation, methods, and interpretation, are you unsure about?
* What particular data, code, proofs, and arguments would you like to see vetted? If it has already been peer-reviewed in some way, why do you think more review is needed?

## Ultimate goals: what are we trying to optimize?&#x20;

Put broadly, we need to consider how this research allows us to achieve our own goals in line with our [Global Priorities Theory of Change flowchart](https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/benefits-and-features/global-priorities-theory-of-change), targeting "ultimate outcomes." The research we select and evaluate should drive positive change in a meaningful way.

One way we might see this process:  “better research & more informative evaluation” → “better decision-making” → “better outcomes” for humanity and for non-human animals (i.e., the survival and flourishing of life and human civilization and values).

_A point to consider_: Do we have other ultimate goals that are not in that chart, e.g., to improve research and knowledge-building for its own sake?

## Prioritizing research to achieve these goals

As we weigh candidate research to prioritize for evaluation, we need to balance _directly having a positive impact_ against building our ability to have an impact _in the future_. &#x20;

### A. Direct impact (‘score goals now’)

Below, we adapt the ["ITN" cause prioritization framework](https://forum.effectivealtruism.org/topics/itn-framework) (popular in effective altruism circles) to assess the direct impact of our evaluations.

**Importance**&#x20;

_What is the direct impact potential of the research?_

This is a massive question many have tried to address (see sketches and links). We respond to uncertainty around this question in a number of ways, including:

* Consulting a range of sources, not only EA-linked sources.&#x20;
  * EA and more or less adjacent:  [Agendas](https://effective-giving-marketing.gitbook.io/the-unjournal-project-and-communication-space/the-field-and-ea-gp-research/what-is-ea-gp-relevant-research) and overviews, [Syllabi](https://effective-giving-marketing.gitbook.io/economics-for-ea-and-vice-versa/existing-resources-programs-examples).
  * Non-EA, e.g., [https://globalchallenges.org/](https://globalchallenges.org/).
* Scoping what other sorts of work are representative inputs to GP-relevant work.&#x20;
  * Get a selection of seminal GP publications; look back to see what they are citing and categorize by journal/field/keywords/etc.&#x20;

**Neglectedness**&#x20;

Where is the current journal system failing GP-relevant work the most . . . in ways we can address?

**Tractability**

1. “Evaluability” of research: Where does the UJ _approach_ yield the most insight or value of information?
2. Existing expertise: Where do we have field expertise on the UJ team? This will help us commission stronger evaluations.
3. "Feedback loops": Could this research influence concrete intervention choices? Does it predict near-term outcomes? If so, observing these choices and outcomes and getting feedback on the research and our evaluation can yield strong benefits.&#x20;

_Consideration/discussion_: How much should we include research with _indirect_ impact potential (theoretical, methodological, etc.)?

### B. Sustainability: funding, support, participation

Moreover, we need to consider how the research evaluation might support the sustainability of _The Unjournal_ and the broader general project of open evaluation. We might want to strike a balance in terms of including cause areas of greatest interest from various quarters, considering...

* Relevance to stakeholders and potential supporters
* Clear connections to impact; measurability
* Support from relevant academic communities
* Support from open science

_Consideration/discussion_: What will drive further interest and funding?&#x20;

### C. Credibility, visibility, and driving positive institutional chage

Finally, we look to ways in which particular approaches can increase visibility and solidify the credibility of _The Unjournal_ and open evaluations. We consider the extent to which our choices may help drive positive institutional change.

* Interest and involve academics—and build the status of the project.
* Commission evaluations that will be visibly useful and credible.
* ‘Benchmark traditional publication outcomes’ and track our predictiveness and impact.
* Have strong leverage over research "outcomes and rewards."
* Increase public visibility and raise public interest.
* Bring in supporters and participants.
* Achieve substantial output in a reasonable time frame and with reasonable expense.
* Maintain goodwill and the justified reputation for being fair and impartial.

<details>

<summary>But some of these concerns may have trade offs</summary>

We are aware of possible pitfalls of some elements of our vision.

We’ve discussed a second "non-academic, high-impact policy work" track for evaluation. This may have direct impact and please SFF funders, but may distract us from changing academic systems, and may cost us status in academia (if not done carefully).&#x20;

A focus on topics perceived as niche (e.g., the economics and game theory of AI governance and AI safety) may bring a similar tradeoff.

On the other hand, perhaps a focus on behavioral and experimental economics would generate lots of academic interest and participants; this could help us benchmark our evaluations, etc.; but this may also be less directly impactful.

Giving managers autonomy and pushing forward quickly may bring the risk of perceived favoritism; a rule-based systematic approach to choosing papers to evaluate might be slower and less interesting for managers. However, it might be seen as fairer (and it might enable better measurement of our impact).

</details>

Overall, however, we feel we are aware of key issues around the consideration of research to be evaluated in _The Unjournal_ and continue to hone and improve our process for approval.

##

## Data:  what are we evaluating/considering?

We're working on presenting and analyzing the specifics surrounding our current evaluation data [here](https://unjournal.github.io/unjournaldata/chapters/evaluation\_data\_analysis.html).&#x20;



## Choosing papers to pilot

### A (slightly earlier) template for suggesting/submitting  papers

Below, we present a both. for our own consideration and for sharing (in part?) with evaluators the referees, to give [them some guidance](#user-content-fn-5)[^5]. Think of these as **bespoke evaluation notes** for a ["research overview, prioritization, and suggestions" document](#user-content-fn-6)[^6].&#x20;

<details>

<summary>Proposed template</summary>

#### Title

One-click-link\
Link to any private hosted comments on the paper/project

#### Summary; why is this research relevant and worth engaging with?

As mentioned above under _High level considerations_, consider factors including importance to global priorities, relevance to the field, the commitment and contribution to open science, the authors’ engagement, and the transparency of data and reasoning. You may consider the [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework-1) explicitly, but not too rigidly.

#### Why does it need (more) review, and what are some key issues and claims to vet?

What are (some of) the authors’ main important claims that are worth carefully evaluating? What aspects of the evidence, argumentation, methods, interpretation, etc., are you unsure about? What particular data, code, proof, etc., would you like to see vetted? If it has already been peer-reviewed in some way, why do you think more review is needed?

#### What sort of reviewers should be sought, and what should they be asked?

What types of expertise and background would be most appropriate for the evaluation? Who would be interested? Please try to make specific suggestions.

#### How well has the author engaged with the process?

Do they need particular convincing? Do they need help making their engagement with _The Unjournal_ successful?

</details>



\
\
\




###

[^1]: We discuss our prioritization considerations under [global-priorities.md](../../faq-interaction/global-priorities.md "mention")

[^2]: We target these areas (1) because of our current management team's expertise and (2) because these seem particularly in need of The Unjournal's approach. However, we are open to expanding and branching out.



[^3]: Particularly where these evaluations are public, or there are clear public signals that research users can use to make reasonably make inferences about the research quality, credibility, and usefulness.  E.g., to some extent we might consider publication in a journal known for particularly high standards as such a signal. Replication (e.g., via i4replication.org) would also often be a strong signal.

[^4]: Our evaluations involve many hours of work from researchers with very specific expertise in the narrow research area and methods. In contrast, our prioritization is less intensive, and the field specialists will tend to have adjacent expertise.&#x20;

[^5]: We probably won’t need to offer quite as much guidance in equilibrium, but it may be worth a little more care while the process is piloted. &#x20;



[^6]: The evaluation manager ideally creates this document to share with the evaluators (view-only perhaps, so anonymous evaluators don’t accidentally disclose identity), explaining why _The Unjournal_ chose this paper, what we are looking for in evaluators/evaluations, and suggesting specific things we might like the evaluators to focus on.

---
description: (for pilot and beyond)
---

# What research to target?

Our [initial focus](#user-content-fn-1)[^1] is quantitative work that informs [global priorities (see linked discussion)](../../the-field-and-ea-gp-research.md), especially in [economics, policy, and social science](#user-content-fn-2)[^2]. We want to see better research leading to better outcomes in the real world (see our '[Theory of Change](../../benefits-and-features/global-priorities-theory-of-change/)').

{% hint style="info" %}
_See (earlier) discussion in public call/EA forum discussion_ [_HERE_](https://forum.effectivealtruism.org/posts/kftzYdmZf4nj2ExN7/the-most-pivotal-empirical-pieces-of-research-you-would-like-2)_._
{% endhint %}

To reach these goals, we need to select "the right research" for evaluation. We want to choose papers and projects that are highly relevant, methodologically promising, and that will benefit substantially from our evaluation work. We need to optimize how we select research so that our efforts remain mission-focused and _useful_. We also want to make our process transparent and fair. To do this, we are building a coherent set of criteria and goals, and a specific approach to guide this process. We explore several dimensions of these criteria below.

{% hint style="info" %}
_Management access only_: General discussion of prioritization in Gdoc [HERE](https://docs.google.com/document/d/1Ei-3t2bUazjlBOIg8chLxOrz\_sXl\_osAzrLU4hO1wDk/edit). Private discussion of specific papers in Airtable and links (e.g., [HERE](https://docs.google.com/document/d/14HXHQTqwJ5VOw-SBoJD8Sd3jathdO9geKdmhdOOx\_Gw/edit)). _We incorporate some of this discussion below._
{% endhint %}

## High-level considerations for prioritizing research

When considering a piece of research to decide whether to commission it to be evaluated, we can start by looking at its _general relevance_ as well as the _value_ _of evaluating_ and rating it.

{% hint style="info" %}
Our prioritization of a paper for evaluation should not be seen as an assessment of its quality, nor of its 'vulnerability'.   Furthermore, ['the prioritization is not the evaluation', it is less ](#user-content-fn-3)[^3]specific and less intensive.
{% endhint %}

1. **Why is it relevant and worth engaging with?**

We consider (and _prioritize_) the importance of the research to global priorities; its relevance to crucial decisions;  the attention it is getting, the influence it is having; its direct relevance to the real world; and the potential value of the research for advancing other impactful work.  We _de-prioritize_ work that has already been credibly (publicly) evaluated[^4].  We also consider the fit of the research with our scope (social science, etc.), and the likelihood that we can commission experts to meaningfully evaluate it. As noted [below](what-research-to-target.md#b.-sustainability-funding-support-participation), some 'instrumental goals' ([sustainability](what-research-to-target.md#b.-sustainability-funding-support-participation), [building credibility](what-research-to-target.md#c.-credibility-visibility-and-driving-positive-institutional-change), driving change, ...) also play a role in our choices.

_**Some features we value, that might raise the probability we consider a paper or project include**_ the commitment and contribution to open science, the authors' engagement with our process, and the logic, communication, and transparent reasoning of the work. _**However**,_ if a prominent research paper is within our scope and seems to have a strong potential for impact, we will prioritize it highly, whether or not it has these qualities.

\
2\. **Why does it need (more) evaluation, and what are some key issues and claims to vet?**

_We ask the people who suggest particular research, and experts in the field:_

* What are (some of) the authors’ key/important claims that are worth evaluating?
* What aspects of the evidence, argumentation, methods, and interpretation, are you unsure about?
* What particular data, code, proofs, and arguments would you like to see vetted? If it has already been peer-reviewed in some way, why do you think _more_ review is needed?

## Ultimate goals: what are we trying to optimize?&#x20;

Put broadly, we need to consider how this research allows us to achieve our own goals in line with our [Global Priorities Theory of Change flowchart](https://effective-giving-marketing.gitbook.io/unjournal-x-ea-and-global-priorities-research/benefits-and-features/global-priorities-theory-of-change), [targeting "ultimate outcomes."](#user-content-fn-5)[^5] The research we select and evaluate should meaningfully drive positive change. One way we might see this process:  “better research & more informative evaluation” → “better decision-making” → “better outcomes” for humanity and for non-human animals (i.e., the survival and flourishing of life and human civilization and values).&#x20;

## Prioritizing research to achieve these goals

As we weigh research to prioritize for evaluation, we need to balance _directly having a positive impact_ against building our ability to have an impact _in the future_. &#x20;

### A. Direct impact (‘score goals now’)

Below, we adapt the ["ITN" cause prioritization framework](https://forum.effectivealtruism.org/topics/itn-framework) (popular in effective altruism circles) to assess the direct impact of our evaluations.

**Importance**&#x20;

_What is the direct impact potential of the research?_

This is a massive question many have tried to address (see sketches and links below). We respond to uncertainty around this question in several ways, including:

* Consulting a range of sources, not only EA-linked sources.&#x20;
  * EA and more or less adjacent:  [Agendas](https://effective-giving-marketing.gitbook.io/the-unjournal-project-and-communication-space/the-field-and-ea-gp-research/what-is-ea-gp-relevant-research) and overviews, [Syllabi](https://effective-giving-marketing.gitbook.io/economics-for-ea-and-vice-versa/existing-resources-programs-examples).
  * Non-EA, e.g., [https://globalchallenges.org/](https://globalchallenges.org/).
* Scoping what other sorts of work are representative inputs to GP-relevant work.&#x20;
  * Get a selection of seminal GP publications; look back to see what they are citing and categorize by journal/field/keywords/etc.&#x20;

**Neglectedness**&#x20;

Where is the current journal system failing GP-relevant work the most . . . in ways we can address?

**Tractability**

1. “Evaluability” of research: Where does the _UJ_ _approach_ yield the most insight or value of information?
2. Existing expertise: Where do we have field expertise on the UJ team? This will help us commission stronger evaluations.
3. "Feedback loops": Could this research influence concrete intervention choices? Does it predict near-term outcomes? If so, observing these choices and outcomes and getting feedback on the research and our evaluation can yield strong benefits.&#x20;

_Consideration/discussion_: How much should we include research with _indirect_ impact potential (theoretical, methodological, etc.)?

### B. Sustainability: funding, support, participation

Moreover, we need to consider how the research evaluation might support the sustainability of _The Unjournal_ and the broader general project of open evaluation. We may need to strike a balance between work informing the priorities of various audences, including:

* Relevance to stakeholders and potential supporters
* Clear connections to impact; measurability
* Support from relevant academic communities
* Support from open science

_Consideration/discussion_: What will drive further interest and funding?&#x20;

### C. Credibility, visibility, driving positive institutional change

Finally, we consider how our choices will increase the visibility and solidify the credibility of _The Unjournal_ and open evaluations. We consider how our work may help drive positive institutional change. We aim to:

* Interest and involve academics—and build the status of the project.
* Commission evaluations that will be visibly useful and credible.
* ‘Benchmark traditional publication outcomes’, track our predictiveness and impact.
* Have strong leverage over research "outcomes and rewards."
* Increase public visibility and raise public interest.
* Bring in supporters and participants.
* Achieve substantial output in a reasonable time frame and with reasonable expense.
* Maintain goodwill and a justified reputation for being fair and impartial.

<details>

<summary>But some of these concerns may have trade offs</summary>

We are aware of possible pitfalls of some elements of our vision.

We are pursuing a second "high-impact policy and applied research" track for evaluation.  This will consider work that is not targeted at academic audiences. This may have direct impact and please SFF funders, but, if not done carefully, this may distract us from changing academic systems, and may cost us status in academia.&#x20;

A focus on topics perceived as niche (e.g., the economics and game theory of AI governance and AI safety) may bring a similar tradeoff.

On the other hand, perhaps a focus on behavioral and experimental economics would generate lots of academic interest and participants; this could help us benchmark our evaluations, etc.; but this may also be less directly impactful.

Giving managers autonomy and pushing forward quickly may bring the risk of perceived favoritism; a rule-based systematic approach to choosing papers to evaluate might be slower and less interesting for managers. However, it might be seen as fairer (and it might enable better measurement of our impact).

</details>

We hope we have identified the important considerations (above); but we may be missing key points. We continue to engage discussion and seek feedback, to _hone and improve our processes and approaches._

## Data:  what are we evaluating/considering?

We present and analyze the specifics surrounding our current evaluation data in [this interactive notebook/dashboard here. ](https://unjournal.github.io/unjournaldata/chapters/evaluation\_data\_analysis.html)



{% hint style="info" %}
**Below**: An earlier template for considering and discussing the relevance of research. This was/is provided both for our own consideration and for sharing (in part?) with evaluators, to give [them some guidance](#user-content-fn-6)[^6]. Think of these as **bespoke evaluation notes** for a ["research overview, prioritization, and suggestions" document](#user-content-fn-7)[^7].&#x20;
{% endhint %}

<details>

<summary>Proposed template</summary>

#### Title

* One-click-link to paper
* Link to any private hosted comments on the paper/project

#### Summary; why is this research relevant and worth engaging with?

As mentioned under [_High level considerations_](what-research-to-target.md#high-level-considerations-for-prioritizing-research), consider factors including importance to global priorities, relevance to the field, the commitment and contribution to open science, the authors’ engagement, and the transparency of data and reasoning. You may consider the [ITN framework](https://forum.effectivealtruism.org/topics/itn-framework-1) explicitly, but not too rigidly.

#### Why does it need (more) review, and what are some key issues and claims to vet?

What are (some of) the authors’ main important claims that are worth carefully evaluating? What aspects of the evidence, argumentation, methods, interpretation, etc., are you unsure about? What particular data, code, proof, etc., would you like to see vetted? If it has already been peer-reviewed in some way, why do you think more review is needed?

#### What sort of reviewers should be sought, and what should they be asked?

What types of expertise and background would be most appropriate for the evaluation? Who would be interested? Please try to make specific suggestions.

#### How well has the author engaged with the process?

Do they need particular convincing? Do they need help making their engagement with _The Unjournal_ successful?

</details>



[^1]: We discuss our prioritization considerations under [global-priorities.md](../../faq-interaction/global-priorities.md "mention")

[^2]: We target these areas (1) because of our current management team's expertise and (2) because these seem particularly in need of The Unjournal's approach. However, we are open to expanding and branching out.



[^3]: Our evaluations involve many hours of work from researchers with very specific expertise in the narrow research area and methods. In contrast, our prioritization is less intensive, and the field specialists will tend to have adjacent expertise.&#x20;

[^4]: Particularly where these evaluations are public, or there are clear public signals that research users can use to make reasonably make inferences about the research quality, credibility, and usefulness.  E.g., to some extent we might consider publication in a journal known for particularly high standards as such a signal. Replication (e.g., via i4replication.org) would also often be a strong signal.

[^5]: _We are still considering_: Do we have _other_ ultimate goals (outside of that chart) e.g., to improve research and knowledge-building for its own sake?

[^6]: We probably won’t need to offer quite as much guidance in equilibrium, but it may be worth a little more care while the process is piloted. &#x20;



[^7]: The evaluation manager ideally creates this document to share with the evaluators (view-only perhaps, so anonymous evaluators don’t accidentally disclose identity), explaining why _The Unjournal_ chose this paper, what we are looking for in evaluators/evaluations, and suggesting specific things we might like the evaluators to focus on.

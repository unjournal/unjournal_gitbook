# (Communicating results)

## Curating and publishing evaluations, linked to research

* Sciety Group: curating evaluations and papers
* Evaluations and author response given DOI's, enter the bibliometric record
  * Considering: 'publication tier' of authors' responses as a workaround to encode aggregated evaluation &#x20;
* Hypothes.is annotation of hosted and linked papers and projects
* Sharing evaluation data&#x20;

## Aggregating evaluators' ratings and predictions

We aim to elicit the experiment judgment from Unjournal evaluators efficiently and precisely. We aim to communicate this quantitative information concisely and usefully, in ways that will inform policymakers, philanthropists, and future researchers.&#x20;

In the short run/in our pilot phase, we will attempt to present simple but reasonable aggregations, such as simple averages of midpoints and confidence-interval bounds. However, going forward, we are consulting and incorporating the burgeoning academic literature on 'aggregating expert opinion'. (See, e.g.,  [Hemming et al, 2017](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.12857); [Hanea et al, 2021](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8412308/); [McAndrew et al, 2020](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7996321/); [Marcoci et al, 2022](https://bmcresnotes.biomedcentral.com/articles/10.1186/s13104-022-06016-0))

## Other communication

Considering...

* Syntheses of evaluations and author feedback
* Input to prediction markets, replication projects, etc.&#x20;
* Less technical summaries and policy-relevant summaries, e.g., for the [EA Forum](https://forum.effectivealtruism.org/), [Asterisk magazine](https://asteriskmag.com/), or mainstream long-form outlets





